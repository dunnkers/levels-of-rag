{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "result = llm.invoke(\"hello!\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009113337844610214"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "embedding.embed_query(\"dog\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unfortunately, I do not have access to a list of specific talks for PyData Eindhoven 2024. \n",
       "\n",
       "* **Conference websites are your best bet:**  Keep an eye on the official PyData Eindhoven website. They will publish a schedule closer to the event date. \n",
       "* **Look for past trends:** You can also search for talk titles and descriptions from previous PyData Eindhoven conferences. This might give you an idea of the types of sports-related topics covered in the past.\n",
       "\n",
       "Good luck finding the information you're looking for! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm.invoke(\n",
    "    \"what talks are given at PyData Eindhoven 2024 about sports?\"\n",
    ").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PyData schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = pd.read_json(\"data/pydata_eindhoven_2024_sessions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Explainable AI in the LIME-light\n",
       "LIME, a model-agnostic AI framework, illuminates the path to local explainability, primarily for classification models. Delving into the theory underpinning LIME, we explore diverse use cases and its adaptability across various scenarios. Through practical examples, we showcase the breadth of applications for LIME. By the presentation's conclusion, you'll have gained insights into leveraging LIME to clarify individual prediction logic, leading to more accessible explanations.\n",
       "\n",
       "## Description\n",
       "Although AI toolkits have simplified model implementation, understanding and interpreting these models remain challenging. With regulatory frameworks like the EU AI Act emphasizing explainability, the need for tools like LIME is paramount.\r\n",
       "\r\n",
       "This presentation will provide an in-depth overview of LIME (Local Interpretable Model-agnostic Explanations), highlighting its utility in facilitating model comprehension. No prior expertise is assumed. Beginning with an explanation of LIME's theory and its practical implementation in Python, we'll then delve into diverse classification scenarios to showcase LIME's effectiveness. Additionally, we'll explore how the original LIME framework has been extended to handle time series data.\n",
       "\n",
       "## Timeslot\n",
       "2024-07-11T10:00:00+02:00 with a duration of 00:30\n",
       "\n",
       "## Room\n",
       "If (1.1)\n",
       "\n",
       "## Speaker\n",
       " ### Sanne van den Bogaart\n",
       "For the past 3 years I have been working as a Data Science consultant at Pipple. Since Pipple is active in multiple different sectors, I have had the opportunity to do many different projects. What I have discovered is that explainability of the machine learning used was a critical topic in all of these projects. Fortunately, frameworks like LIME have emerged to provided this much needed explainability. I am excited to discuss more about LIME at the upcoming 2024 PyData Eindhoven conference.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(sessions[\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# Explainable AI in the LIME-light\\nLIME, a model-agnostic AI framework, illuminates the path to local explainability, primarily for classification models. Delving into the theory underpinning LIME, we explore diverse use cases and its adaptability across various scenarios. Through practical examples, we showcase the breadth of applications for LIME. By the presentation's conclusion, you'll have gained insights into leveraging LIME to clarify individual prediction logic, leading to more accessible explanations.\\n\\n## Description\\nAlthough AI toolkits have simplified model implementation, understanding and interpreting these models remain challenging. With regulatory frameworks like the EU AI Act emphasizing explainability, the need for tools like LIME is paramount.\\r\\n\\r\\nThis presentation will provide an in-depth overview of LIME (Local Interpretable Model-agnostic Explanations), highlighting its utility in facilitating model comprehension. No prior expertise is assumed. Beginning with an explanation of LIME's theory and its practical implementation in Python, we'll then delve into diverse classification scenarios to showcase LIME's effectiveness. Additionally, we'll explore how the original LIME framework has been extended to handle time series data.\\n\\n## Timeslot\\n2024-07-11T10:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Sanne van den Bogaart\\nFor the past 3 years I have been working as a Data Science consultant at Pipple. Since Pipple is active in multiple different sectors, I have had the opportunity to do many different projects. What I have discovered is that explainability of the machine learning used was a critical topic in all of these projects. Fortunately, frameworks like LIME have emerged to provided this much needed explainability. I am excited to discuss more about LIME at the upcoming 2024 PyData Eindhoven conference.\\n\\n\", metadata={'guid': 'a2eb19a3-1642-5781-bb82-45b681556560', 'logo': '', 'date': Timestamp('2024-07-11 10:00:00+0200', tz='UTC+02:00'), 'start': '10:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-17-explainable-ai-in-the-lime-light', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/QFW9XN/', 'title': 'Explainable AI in the LIME-light', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"LIME, a model-agnostic AI framework, illuminates the path to local explainability, primarily for classification models. Delving into the theory underpinning LIME, we explore diverse use cases and its adaptability across various scenarios. Through practical examples, we showcase the breadth of applications for LIME. By the presentation's conclusion, you'll have gained insights into leveraging LIME to clarify individual prediction logic, leading to more accessible explanations.\", 'description': \"Although AI toolkits have simplified model implementation, understanding and interpreting these models remain challenging. With regulatory frameworks like the EU AI Act emphasizing explainability, the need for tools like LIME is paramount.\\r\\n\\r\\nThis presentation will provide an in-depth overview of LIME (Local Interpretable Model-agnostic Explanations), highlighting its utility in facilitating model comprehension. No prior expertise is assumed. Beginning with an explanation of LIME's theory and its practical implementation in Python, we'll then delve into diverse classification scenarios to showcase LIME's effectiveness. Additionally, we'll explore how the original LIME framework has been extended to handle time series data.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 25, 'code': 'LHNM7K', 'public_name': 'Sanne van den Bogaart', 'biography': 'For the past 3 years I have been working as a Data Science consultant at Pipple. Since Pipple is active in multiple different sectors, I have had the opportunity to do many different projects. What I have discovered is that explainability of the machine learning used was a critical topic in all of these projects. Fortunately, frameworks like LIME have emerged to provided this much needed explainability. I am excited to discuss more about LIME at the upcoming 2024 PyData Eindhoven conference.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Sanne van den Bogaart\\nFor the past 3 years I have been working as a Data Science consultant at Pipple. Since Pipple is active in multiple different sectors, I have had the opportunity to do many different projects. What I have discovered is that explainability of the machine learning used was a critical topic in all of these projects. Fortunately, frameworks like LIME have emerged to provided this much needed explainability. I am excited to discuss more about LIME at the upcoming 2024 PyData Eindhoven conference.\\n'}),\n",
       " Document(page_content=\"# Risks and Mitigations for a Safe and Responsible AI\\nThis talk will explain the multifaceted risks associated with building custom AI solutions, both from Responsible AI and Safety perspectives, and explore ways to mitigate them, to ensure that as AI professionals we create non-harmful AI systems.\\n\\n## Description\\nThe rapid advancement of AI technologies, especially in the LLM space, is opening countless opportunities across all industries to apply in their daily business. However, it also introduces significant risks that could impact their users and broader society, from ethical and safety perspectives.\\r\\n\\r\\nIn this talk we will delve into various aspects to consider when building custom AI solutions to ensure they are not harmful in any way. We'll explain the types of risks to assess from both Responsible AI and Safety perspectives, and what mitigations can be implemented to address them. We'll discuss broad aspects that apply to all AI systems, such as fairness and inclusiveness, as well as risks specific to Large Language Models, like prompt injection attacks and hallucinations.\\r\\n \\r\\nBy the end of this talk, participants will have a clear understanding of AI risks, and a practical framework for evaluating and implementing responsible AI practices when building AI-based applications.\\r\\n \\r\\nThe talk is designed for anyone involved in the design and development of AI systems, from AI developers and data scientists to project managers. No technical knowledge is required, but a prior understanding of the fundamentals of AI and LLMs is assumed.\\n\\n## Timeslot\\n2024-07-11T11:15:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Maria Medina\\nMaria works as a Senior Data Scientist at Microsoft, currently based in The Netherlands. Her background is in computer science and mathematics and has 10+ years experience in data science consulting, using applied AI to solve business challenges in several industries and countries. She is also an advocate for diversity in technology and a former co-organizer of the PyLadies Madrid community.\\n\\n\", metadata={'guid': '360bdd08-ebe7-5569-9654-59b2c15e9505', 'logo': '', 'date': Timestamp('2024-07-11 11:15:00+0200', tz='UTC+02:00'), 'start': '11:15', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-64-risks-and-mitigations-for-a-safe-and-responsible-ai', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/ZWTNSH/', 'title': 'Risks and Mitigations for a Safe and Responsible AI', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'This talk will explain the multifaceted risks associated with building custom AI solutions, both from Responsible AI and Safety perspectives, and explore ways to mitigate them, to ensure that as AI professionals we create non-harmful AI systems.', 'description': \"The rapid advancement of AI technologies, especially in the LLM space, is opening countless opportunities across all industries to apply in their daily business. However, it also introduces significant risks that could impact their users and broader society, from ethical and safety perspectives.\\r\\n\\r\\nIn this talk we will delve into various aspects to consider when building custom AI solutions to ensure they are not harmful in any way. We'll explain the types of risks to assess from both Responsible AI and Safety perspectives, and what mitigations can be implemented to address them. We'll discuss broad aspects that apply to all AI systems, such as fairness and inclusiveness, as well as risks specific to Large Language Models, like prompt injection attacks and hallucinations.\\r\\n \\r\\nBy the end of this talk, participants will have a clear understanding of AI risks, and a practical framework for evaluating and implementing responsible AI practices when building AI-based applications.\\r\\n \\r\\nThe talk is designed for anyone involved in the design and development of AI systems, from AI developers and data scientists to project managers. No technical knowledge is required, but a prior understanding of the fundamentals of AI and LLMs is assumed.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 78, 'code': 'HRQY9D', 'public_name': 'Maria Medina', 'biography': 'Maria works as a Senior Data Scientist at Microsoft, currently based in The Netherlands. Her background is in computer science and mathematics and has 10+ years experience in data science consulting, using applied AI to solve business challenges in several industries and countries. She is also an advocate for diversity in technology and a former co-organizer of the PyLadies Madrid community.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Maria Medina\\nMaria works as a Senior Data Scientist at Microsoft, currently based in The Netherlands. Her background is in computer science and mathematics and has 10+ years experience in data science consulting, using applied AI to solve business challenges in several industries and countries. She is also an advocate for diversity in technology and a former co-organizer of the PyLadies Madrid community.\\n'}),\n",
       " Document(page_content=\"# Predicting the Spring Classics of cycling with my first neural network\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\n\\n## Description\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\\n\", metadata={'guid': '03ccd5af-7a72-58de-acbf-200dc5891883', 'logo': '', 'date': Timestamp('2024-07-11 12:00:00+0200', tz='UTC+02:00'), 'start': '12:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-25-predicting-the-spring-classics-of-cycling-with-my-first-neural-network', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/7A7YQC/', 'title': 'Predicting the Spring Classics of cycling with my first neural network', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.', 'description': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 34, 'code': '9WUZE9', 'public_name': 'Rob Claessens', 'biography': \"I'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\"}),\n",
       " Document(page_content='# Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches\\nThrough single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.\\n\\n## Description\\nTennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n\\n', metadata={'guid': '795350ad-6bec-5f36-b011-fb242a977bef', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-57-computer-vision-at-the-dutch-tennis-federation-utilizing-yolo-to-create-insights-for-coaches', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/UGNXZY/', 'title': 'Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Through single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.', 'description': 'Tennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 40, 'code': 'VRCBRB', 'public_name': 'Max Brouwer', 'biography': 'Max works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.', 'answers': []}, {'id': 70, 'code': 'ATL7DQ', 'public_name': 'Matthias Kempe', 'biography': 'Dr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n'}),\n",
       " Document(page_content=\"# Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.\\nLearn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.\\n\\n## Description\\nIn the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\\r\\nThe presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Yannis MOUDERE\\nI am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\\n\\n\", metadata={'guid': '2c108787-14a0-5b3b-91df-80d011479465', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-54-enhancing-event-analysis-at-scale-leveraging-tracking-data-in-sports-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/BWC9DD/', 'title': 'Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Learn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.', 'description': 'In the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\\r\\nThe presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 68, 'code': 'BCNSWC', 'public_name': 'Yannis MOUDERE', 'biography': \"I am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Yannis MOUDERE\\nI am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\\n\"}),\n",
       " Document(page_content=\"# How I lost 1000€ betting on CS:GO with machine learning and Python\\nPeople have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\\n\\n## Description\\nThis presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\\n\", metadata={'guid': 'aa661e50-ef5d-5d3a-8b84-2d798207eca0', 'logo': '', 'date': Timestamp('2024-07-11 16:05:00+0200', tz='UTC+02:00'), 'start': '16:05', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-31-how-i-lost-1000-betting-on-cs-go-with-machine-learning-and-python', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/WP7S3A/', 'title': 'How I lost 1000€ betting on CS:GO with machine learning and Python', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"People have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\", 'description': \"This presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 42, 'code': 'D7YTNK', 'public_name': 'Pedro Tabacof', 'biography': \"Pedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\"}),\n",
       " Document(page_content='# 🌳 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery 🛰️\\nA case study of how we use Deep Learning based photogrammetry to calculate the height of trees from very high resolution satellite imagery. We show the substantial improvement achieved by switching from classical photogrammetric techniques to a deep learning based model (implemented in PyTorch), and the challenges we had to overcome to make this solution work.\\n\\n## Description\\nThe risk that a tree poses to line infrastructure (such as power lines) is determined by several factors, chief among them the height of the particular tree. The increasing availability of very high resolution satellite imagery makes it possible to use photogrammetric techniques to extract height information from a set of stereo satellite images. By using satellite imagery we can achieve a scale not possible by manual measurement. \\r\\nWe found that classical techniques perform poorly on vegetation, and were handily outperformed by deep learning based techniques implemented in PyTorch. This improvement was not trivial to achieve however, as creating labelled data in sufficient quantity was quite challenging. By increasing the quality of our height predictions we were able to more accurately calculate risk for our customers.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Ferdinand Schenck\\nI am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.\\n\\n', metadata={'guid': 'b2343a84-726c-599a-8ef4-c1d3992a6ded', 'logo': '', 'date': Timestamp('2024-07-11 16:50:00+0200', tz='UTC+02:00'), 'start': '16:50', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-40--the-taller-the-tree-the-harder-the-fall-determining-tree-height-from-space-using-deep-learning-and-very-high-resolution-satellite-imagery-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/XXCFDM/', 'title': '🌳 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery 🛰️', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'A case study of how we use Deep Learning based photogrammetry to calculate the height of trees from very high resolution satellite imagery. We show the substantial improvement achieved by switching from classical photogrammetric techniques to a deep learning based model (implemented in PyTorch), and the challenges we had to overcome to make this solution work.', 'description': 'The risk that a tree poses to line infrastructure (such as power lines) is determined by several factors, chief among them the height of the particular tree. The increasing availability of very high resolution satellite imagery makes it possible to use photogrammetric techniques to extract height information from a set of stereo satellite images. By using satellite imagery we can achieve a scale not possible by manual measurement. \\r\\nWe found that classical techniques perform poorly on vegetation, and were handily outperformed by deep learning based techniques implemented in PyTorch. This improvement was not trivial to achieve however, as creating labelled data in sufficient quantity was quite challenging. By increasing the quality of our height predictions we were able to more accurately calculate risk for our customers.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 53, 'code': 'EJCLAA', 'public_name': 'Ferdinand Schenck', 'biography': 'I am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Ferdinand Schenck\\nI am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.\\n'}),\n",
       " Document(page_content='# BERTopic to accelerate Ukrainian aid by the Red Cross\\nBy means of Topic Modeling, discussed topics can be subtracted from a set of documents. BERTopic is a way of Topic Modeling that uses Large Language Models. A high-level overview of how BERTopic works will be presented, together with its evaluation and the application of it on a use case of the Netherlands Red Cross. In this use case, BERTopic supports in getting insights into the needs of Ukrainian refugees as expressed through social media.  This presentation will help in understanding BERTopic and might inspire for other valuable use cases within your own field.\\n\\n## Description\\n510, the Data & Digital initiative of the Netherlands Red Cross, is performing Social Media Listening on Telegram channels of Ukrainian refugees, to obtain insights in their needs. As a result, the Red Cross can direct its aid more purposefully. Obtaining such insights can be a time-intensive task, while in case of an emergency, one would like to initialize aid as quickly as possible. BERTopic has shown to be a powerful tool in quickly obtaining an overview of the discussion on social media. \\r\\n\\r\\nIn this presentation, a high-level explanation of BERTopic will be provided, which is a LLM-based method to obtain topics from a set of documents and categorize them accordingly. Other than basic machine learning knowledge, no further background knowledge is required. Rather than explaining how an LLM works, we will focus on what its task in BERTopic is. Next, we will discuss the evaluation of BERTopic, which is a challenging task, as it is an unsupervised method. Three different metrics are proposed, including an innovative approach that creates a test set with GPT. Finally, the application and results of BERTopic to the described use case will be discussed. This will show the practicality of BERTopic and hopefully inspire for other applications. \\r\\n\\r\\nTime breakdown:\\r\\n- Introduction to the Red Cross use case: 5 min\\r\\n- BERTopic: 10 min\\r\\n- Evaluation: 5 min\\r\\n- Application and results: 5 min\\r\\n- Q&A: 5 min\\n\\n## Timeslot\\n2024-07-11T10:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nina van Diermen\\nI recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.\\n\\n ### Nina van Diermen\\nI recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.\\n\\n', metadata={'guid': '554060c9-cd90-52de-8c63-a534ef68168b', 'logo': '', 'date': Timestamp('2024-07-11 10:00:00+0200', tz='UTC+02:00'), 'start': '10:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-22-bertopic-to-accelerate-ukrainian-aid-by-the-red-cross', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/X7GXYH/', 'title': 'BERTopic to accelerate Ukrainian aid by the Red Cross', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'By means of Topic Modeling, discussed topics can be subtracted from a set of documents. BERTopic is a way of Topic Modeling that uses Large Language Models. A high-level overview of how BERTopic works will be presented, together with its evaluation and the application of it on a use case of the Netherlands Red Cross. In this use case, BERTopic supports in getting insights into the needs of Ukrainian refugees as expressed through social media.  This presentation will help in understanding BERTopic and might inspire for other valuable use cases within your own field.', 'description': '510, the Data & Digital initiative of the Netherlands Red Cross, is performing Social Media Listening on Telegram channels of Ukrainian refugees, to obtain insights in their needs. As a result, the Red Cross can direct its aid more purposefully. Obtaining such insights can be a time-intensive task, while in case of an emergency, one would like to initialize aid as quickly as possible. BERTopic has shown to be a powerful tool in quickly obtaining an overview of the discussion on social media. \\r\\n\\r\\nIn this presentation, a high-level explanation of BERTopic will be provided, which is a LLM-based method to obtain topics from a set of documents and categorize them accordingly. Other than basic machine learning knowledge, no further background knowledge is required. Rather than explaining how an LLM works, we will focus on what its task in BERTopic is. Next, we will discuss the evaluation of BERTopic, which is a challenging task, as it is an unsupervised method. Three different metrics are proposed, including an innovative approach that creates a test set with GPT. Finally, the application and results of BERTopic to the described use case will be discussed. This will show the practicality of BERTopic and hopefully inspire for other applications. \\r\\n\\r\\nTime breakdown:\\r\\n- Introduction to the Red Cross use case: 5 min\\r\\n- BERTopic: 10 min\\r\\n- Evaluation: 5 min\\r\\n- Application and results: 5 min\\r\\n- Q&A: 5 min', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 30, 'code': 'FJBLSK', 'public_name': 'Nina van Diermen', 'biography': 'I recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.', 'answers': []}, {'id': 88, 'code': '9FUMGJ', 'public_name': 'Nina van Diermen', 'biography': 'I recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nina van Diermen\\nI recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.\\n\\n ### Nina van Diermen\\nI recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.\\n'}),\n",
       " Document(page_content=\"# The Levels of RAG 🦜\\nLLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\\n\\n## Description\\nRetrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you 🫵:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk 📌\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. … so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community 🫂\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ❤️ Open Source Software\\r\\nRAG and LLM’s are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\\n\\n## Timeslot\\n2024-07-11T11:15:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n\\n\", metadata={'guid': 'cdc3ef0a-718c-5330-b7cf-a8bf3db2a3f6', 'logo': '/media/cfp/submissions/G8VWGY/Screenshot_2024-06-29_at_17.07.32_W5To5r0.png', 'date': Timestamp('2024-07-11 11:15:00+0200', tz='UTC+02:00'), 'start': '11:15', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-69-the-levels-of-rag-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/G8VWGY/', 'title': 'The Levels of RAG 🦜', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"LLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\", 'description': \"Retrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you 🫵:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk 📌\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. … so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community 🫂\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ❤️ Open Source Software\\r\\nRAG and LLM’s are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 51, 'code': 'YM9JHZ', 'public_name': 'Jeroen Overschie', 'biography': 'Jeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n'}),\n",
       " Document(page_content=\"# Scikit-Learn can do THAT?!\\nMany of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\\n\\n## Description\\nThere may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\\n\", metadata={'guid': 'ab2ef9d4-8af9-54d7-a3fe-272999159d7b', 'logo': '', 'date': Timestamp('2024-07-11 12:00:00+0200', tz='UTC+02:00'), 'start': '12:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-9-scikit-learn-can-do-that-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/3BF9AT/', 'title': 'Scikit-Learn can do THAT?!', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Many of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\", 'description': \"There may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 15, 'code': 'G7KRFK', 'public_name': 'Vincent D. Warmerdam', 'biography': \"Vincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\"}),\n",
       " Document(page_content=\"# Evaluating LLM Frameworks\\nLarge Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\\n\\n## Description\\nAt CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\\n\", metadata={'guid': 'b883c44e-ef8a-5f2f-9fd8-2cb8e958d4a7', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-32-evaluating-llm-frameworks', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/33M979/', 'title': 'Evaluating LLM Frameworks', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Large Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\", 'description': \"At CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 43, 'code': 'LKUKVW', 'public_name': 'Ennia Suijkerbuijk', 'biography': \"Ennia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\"}),\n",
       " Document(page_content='# Maximizing marketplace experimentation: switchback design for small samples and subtle effects\\nConventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.\\n\\n## Description\\nIn this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing\\'s limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic\\'s relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They\\'ll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n\\n', metadata={'guid': '44976e6d-e92c-5a5f-94cd-e32b6c798e5e', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-21-maximizing-marketplace-experimentation-switchback-design-for-small-samples-and-subtle-effects', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/KBYKXY/', 'title': 'Maximizing marketplace experimentation: switchback design for small samples and subtle effects', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.', 'description': \"In this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic's relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They'll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 32, 'code': 'GFKEBB', 'public_name': 'Nazli M. Alagoz', 'biography': 'Naz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.', 'answers': []}, {'id': 86, 'code': 'EL3UZS', 'public_name': 'Joël Gastelaars', 'biography': 'Working on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n'}),\n",
       " Document(page_content='# Causal Forecasting: How to disentangle causal effects, while controlling for unobserved confounders and keeping accuracy\\nA lot of industry-available Machine Learning solutions for causal forecasting have a very particular blind spot: unobserved confounders. We will present an approach that allows you to combine state-of-the-art Machine Learning approaches with advanced Econometrics techniques to get the better of both worlds: accurate causal inference and good forecasting accuracy.\\n\\n## Description\\nCausal Forecasting is a very hot topic in the industry with many applications ranging from marketing spending to pricing. Disentangling causal effects from spurious correlations plays a key role when forecasts are used for decision making, such as in the case of pricing. Solutions available in the industry typically rely on Machine Learning methods that use techniques like DoubleML, Transformers, LSTM, and boosted tree algorithms. A common shortcoming of such solutions is that they do not account for the existence of unobserved confounders, such as world events, or other hard-to-measure effects that can bias the measurement of causal effects. We showcase a solution that was developed over the last 3 years that addresses these challenges by combining advanced Econometrics methods with  ML techniques. The case-study will focus on the example of retail pricing, but the solution is broadly applicable and it has been tested in different settings, including airline pricing.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Marc Nientker\\nMarc Nientker transitioned from a successful seven-year academic career in econometrics, where he contributed as a PhD and Assistant Professor, to the business world to apply his knowledge on a broader scale. He co-founded Acmetric, a strategic data science consultancy that focuses on transforming businesses through data-driven insights.\\r\\n\\r\\nAcmetric specializes in practical applications of econometrics in areas such as pricing, inventory optimization, product allocation, measurement, and more. His expertise supports organizations in understanding and implementing data-centric strategies that naturally lead to more informed decision-making and operational efficiencies.\\n\\n', metadata={'guid': '82194198-ad3c-5909-b0f5-5ad591a2b815', 'logo': '/media/cfp/submissions/8QNFLU/AC_Profile_picture_-_Marc_1_da8pCEz.jpg', 'date': Timestamp('2024-07-11 16:05:00+0200', tz='UTC+02:00'), 'start': '16:05', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-55-causal-forecasting-how-to-disentangle-causal-effects-while-controlling-for-unobserved-confounders-and-keeping-accuracy', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/8QNFLU/', 'title': 'Causal Forecasting: How to disentangle causal effects, while controlling for unobserved confounders and keeping accuracy', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'A lot of industry-available Machine Learning solutions for causal forecasting have a very particular blind spot: unobserved confounders. We will present an approach that allows you to combine state-of-the-art Machine Learning approaches with advanced Econometrics techniques to get the better of both worlds: accurate causal inference and good forecasting accuracy.', 'description': 'Causal Forecasting is a very hot topic in the industry with many applications ranging from marketing spending to pricing. Disentangling causal effects from spurious correlations plays a key role when forecasts are used for decision making, such as in the case of pricing. Solutions available in the industry typically rely on Machine Learning methods that use techniques like DoubleML, Transformers, LSTM, and boosted tree algorithms. A common shortcoming of such solutions is that they do not account for the existence of unobserved confounders, such as world events, or other hard-to-measure effects that can bias the measurement of causal effects. We showcase a solution that was developed over the last 3 years that addresses these challenges by combining advanced Econometrics methods with  ML techniques. The case-study will focus on the example of retail pricing, but the solution is broadly applicable and it has been tested in different settings, including airline pricing.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 69, 'code': 'N733EQ', 'public_name': 'Marc Nientker', 'biography': 'Marc Nientker transitioned from a successful seven-year academic career in econometrics, where he contributed as a PhD and Assistant Professor, to the business world to apply his knowledge on a broader scale. He co-founded Acmetric, a strategic data science consultancy that focuses on transforming businesses through data-driven insights.\\r\\n\\r\\nAcmetric specializes in practical applications of econometrics in areas such as pricing, inventory optimization, product allocation, measurement, and more. His expertise supports organizations in understanding and implementing data-centric strategies that naturally lead to more informed decision-making and operational efficiencies.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Marc Nientker\\nMarc Nientker transitioned from a successful seven-year academic career in econometrics, where he contributed as a PhD and Assistant Professor, to the business world to apply his knowledge on a broader scale. He co-founded Acmetric, a strategic data science consultancy that focuses on transforming businesses through data-driven insights.\\r\\n\\r\\nAcmetric specializes in practical applications of econometrics in areas such as pricing, inventory optimization, product allocation, measurement, and more. His expertise supports organizations in understanding and implementing data-centric strategies that naturally lead to more informed decision-making and operational efficiencies.\\n'}),\n",
       " Document(page_content='# Cloud? No Thanks! I’m Gonna Run GenAI on My AI PC\\nIn this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.\\n\\n## Description\\nIn this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master’s degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he’s a traveler. You can also talk with him about finance, especially investments.\\n\\n', metadata={'guid': 'bf13b897-74b8-5593-88c5-f547b655b25b', 'logo': '', 'date': Timestamp('2024-07-11 16:50:00+0200', tz='UTC+02:00'), 'start': '16:50', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-59-cloud-no-thanks-i-m-gonna-run-genai-on-my-ai-pc', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/SMX9CS/', 'title': 'Cloud? No Thanks! I’m Gonna Run GenAI on My AI PC', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'In this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.', 'description': 'In this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 74, 'code': 'JGCHB3', 'public_name': 'Dmitriy Pastushenkov', 'biography': 'Dmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master’s degree in Computer Science from Moscow Power Engineering Institute (Technical University).', 'answers': []}, {'id': 73, 'code': 'TAYW7K', 'public_name': 'Adrian Boguszewski', 'biography': 'AI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he’s a traveler. You can also talk with him about finance, especially investments.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master’s degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he’s a traveler. You can also talk with him about finance, especially investments.\\n'}),\n",
       " Document(page_content='# Software at ASML: the Force behind making microchips\\nIn a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. In addition, we discuss how ASML engages and cultivates Software talents. At the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future.\\n\\n## Description\\nA full description for this keynote will be announcIn a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. Key highlights include our innovative development environment, which fosters seamless integration and rapid iteration amongst teams. In addition, we discuss how ASML engages and cultivates Software talent through strategic initiatives and collaborative projects. By fostering a dynamic and innovative work environment, we empower our engineers to drive technological advancements and operational excellence.\\r\\n\\r\\nAt the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future of semiconductor fabrication.ed and added to the session when it becomes available.\\n\\n## Timeslot\\n2024-07-11T09:00:00+02:00 with a duration of 00:50\\n\\n## Room\\nREPL (2, mainstage)\\n\\n## Speaker\\n\\n', metadata={'guid': '654f9047-1ee1-57d0-95f7-220b16370422', 'logo': '', 'date': Timestamp('2024-07-11 09:00:00+0200', tz='UTC+02:00'), 'start': '09:00', 'duration': '00:50', 'room': 'REPL (2, mainstage)', 'slug': 'cfp-71-software-at-asml-the-force-behind-making-microchips', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/LKGBYR/', 'title': 'Software at ASML: the Force behind making microchips', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'In a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. In addition, we discuss how ASML engages and cultivates Software talents. At the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future.', 'description': 'A full description for this keynote will be announcIn a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. Key highlights include our innovative development environment, which fosters seamless integration and rapid iteration amongst teams. In addition, we discuss how ASML engages and cultivates Software talent through strategic initiatives and collaborative projects. By fostering a dynamic and innovative work environment, we empower our engineers to drive technological advancements and operational excellence.\\r\\n\\r\\nAt the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future of semiconductor fabrication.ed and added to the session when it becomes available.', 'recording_license': '', 'do_not_record': False, 'persons': [], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ''}),\n",
       " Document(page_content=\"# Sonic Pi - Live Coding as a tool for next-gen education.\\nSonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding.\\n\\n## Description\\nSonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding systems. We'll touch on concurrency, distributed programming, temporal logic, deterministic randomisation, event streams, hot swapping code and domain specific languages.\\r\\n\\r\\nGet ready for some serious live coded beats and a window into an exciting future of computing education.\\n\\n## Timeslot\\n2024-07-11T17:30:00+02:00 with a duration of 01:00\\n\\n## Room\\nREPL (2, mainstage)\\n\\n## Speaker\\n ### Sam Aaron\\nNone\\n\\n\", metadata={'guid': '4b443472-06fa-57a1-9682-a12461da68bf', 'logo': '', 'date': Timestamp('2024-07-11 17:30:00+0200', tz='UTC+02:00'), 'start': '17:30', 'duration': '01:00', 'room': 'REPL (2, mainstage)', 'slug': 'cfp-70-sonic-pi-live-coding-as-a-tool-for-next-gen-education-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/XYLXUP/', 'title': 'Sonic Pi - Live Coding as a tool for next-gen education.', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Sonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding.', 'description': \"Sonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding systems. We'll touch on concurrency, distributed programming, temporal logic, deterministic randomisation, event streams, hot swapping code and domain specific languages.\\r\\n\\r\\nGet ready for some serious live coded beats and a window into an exciting future of computing education.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 87, 'code': 'TBD3DK', 'public_name': 'Sam Aaron', 'biography': None, 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Sam Aaron\\nNone\\n'})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataFrameLoader(sessions, page_content_column=\"text\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"data/basic_rag_vectorstore.faiss\", embeddings=embedding, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to re-create vectors and vectorstore\n",
    "# vectorstore = FAISS.from_documents(docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save\n",
    "# vectorstore.save_local(\"data/basic_rag_vectorstore.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04375917837023735,\n",
       " -0.02100740186870098,\n",
       " -0.008644950576126575,\n",
       " 0.009274132549762726,\n",
       " 0.027995649725198746,\n",
       " 0.056026890873909,\n",
       " 0.05677080899477005,\n",
       " 0.05664321780204773,\n",
       " 0.006435796618461609,\n",
       " -0.017680760473012924,\n",
       " -0.06138936057686806,\n",
       " -0.03150258585810661,\n",
       " -0.00257046427577734,\n",
       " -0.02732081711292267,\n",
       " -0.028928237035870552,\n",
       " 0.02447933331131935,\n",
       " 0.06237539276480675,\n",
       " -0.020569467917084694,\n",
       " -0.09794401377439499,\n",
       " 0.0032784054055809975,\n",
       " -0.05767378211021423,\n",
       " -0.04025986045598984,\n",
       " -0.0580415315926075,\n",
       " 0.006769028026610613,\n",
       " -0.012664892710745335,\n",
       " -0.06807760894298553,\n",
       " 0.007474202197045088,\n",
       " -0.06782187521457672,\n",
       " -0.012073918245732784,\n",
       " -0.032192543148994446,\n",
       " -0.04036087915301323,\n",
       " 0.002051006304100156,\n",
       " 0.04213279113173485,\n",
       " 0.012033850885927677,\n",
       " 0.021148746833205223,\n",
       " 0.016268646344542503,\n",
       " -0.05303194746375084,\n",
       " -0.020383041352033615,\n",
       " 0.04073958843946457,\n",
       " -0.048284076154232025,\n",
       " -0.04711173474788666,\n",
       " 0.007895693182945251,\n",
       " 0.03182351961731911,\n",
       " -0.0037418617866933346,\n",
       " -0.03077150508761406,\n",
       " -0.00536693399772048,\n",
       " 0.00542479008436203,\n",
       " -0.02720167115330696,\n",
       " -0.018719427287578583,\n",
       " 0.036519236862659454,\n",
       " 0.04289570078253746,\n",
       " -0.03867368772625923,\n",
       " -0.07409947365522385,\n",
       " 0.01781950145959854,\n",
       " -0.04366343468427658,\n",
       " 0.0215805321931839,\n",
       " -0.025358472019433975,\n",
       " -0.019284764304757118,\n",
       " 0.032261867076158524,\n",
       " -0.045099105685949326,\n",
       " -0.020947663113474846,\n",
       " 0.004034658893942833,\n",
       " -0.012443309649825096,\n",
       " -0.07467180490493774,\n",
       " 0.034041211009025574,\n",
       " -0.00034859852166846395,\n",
       " -0.04920663312077522,\n",
       " 0.008649887517094612,\n",
       " -0.05318279191851616,\n",
       " 0.001108651515096426,\n",
       " -0.02847132459282875,\n",
       " 0.0811626985669136,\n",
       " -0.011706897988915443,\n",
       " 0.05396808683872223,\n",
       " -0.01865851879119873,\n",
       " -0.021808801218867302,\n",
       " 0.0395936518907547,\n",
       " 0.003868428058922291,\n",
       " -0.004658881109207869,\n",
       " -0.010768159292638302,\n",
       " -0.014320489950478077,\n",
       " 0.02600100077688694,\n",
       " 0.026691684499382973,\n",
       " 0.05512354522943497,\n",
       " 0.017625592648983,\n",
       " 0.0073499176651239395,\n",
       " 0.015056366100907326,\n",
       " 0.002080742036923766,\n",
       " -0.016054416075348854,\n",
       " -0.011583524756133556,\n",
       " 0.03876330330967903,\n",
       " -0.002314352197572589,\n",
       " -0.03464126959443092,\n",
       " -0.005309747532010078,\n",
       " 0.07722540944814682,\n",
       " -0.03198649734258652,\n",
       " -0.042776111513376236,\n",
       " -0.0295754075050354,\n",
       " 0.04554252699017525,\n",
       " 0.06818919628858566,\n",
       " -0.004929111804813147,\n",
       " -0.008210270665585995,\n",
       " 0.06554949283599854,\n",
       " -0.01120687648653984,\n",
       " 0.02678385190665722,\n",
       " 0.04161343351006508,\n",
       " 0.04779000207781792,\n",
       " -0.059225305914878845,\n",
       " 0.04359113425016403,\n",
       " 0.05232441425323486,\n",
       " 0.0061881463043391705,\n",
       " -0.00912664644420147,\n",
       " -0.007149868179112673,\n",
       " -0.031424544751644135,\n",
       " -0.015166155062615871,\n",
       " 0.06962338089942932,\n",
       " 0.007486921735107899,\n",
       " 0.001498999772593379,\n",
       " -0.01643003523349762,\n",
       " 0.008572472259402275,\n",
       " -0.005361130926758051,\n",
       " 0.005235855933278799,\n",
       " -0.0649522915482521,\n",
       " 0.011356078088283539,\n",
       " 0.0257809367030859,\n",
       " -0.05225450545549393,\n",
       " -0.02915015257894993,\n",
       " -0.05557616427540779,\n",
       " -0.10247054696083069,\n",
       " -0.03912773355841637,\n",
       " -0.002310781739652157,\n",
       " -0.11857820302248001,\n",
       " 0.027042878791689873,\n",
       " -0.04582982137799263,\n",
       " -0.05551882088184357,\n",
       " -0.04192153364419937,\n",
       " 0.058026935905218124,\n",
       " -0.02069454826414585,\n",
       " 0.017929770052433014,\n",
       " -0.03324383869767189,\n",
       " -0.021746758371591568,\n",
       " 0.007028951309621334,\n",
       " -0.022357627749443054,\n",
       " 0.04737083241343498,\n",
       " -0.0029853847809135914,\n",
       " -0.017158253118395805,\n",
       " 0.015898948535323143,\n",
       " -0.0011660995660349727,\n",
       " -0.03619060665369034,\n",
       " -0.014734428375959396,\n",
       " -0.04615664482116699,\n",
       " -0.016910064965486526,\n",
       " 0.008596840314567089,\n",
       " -0.02138040028512478,\n",
       " -0.06824740022420883,\n",
       " -0.0029011587612330914,\n",
       " 0.05225043371319771,\n",
       " -0.029822589829564095,\n",
       " 0.10573970526456833,\n",
       " -0.009572051465511322,\n",
       " 0.06390703469514847,\n",
       " -0.07749232649803162,\n",
       " -0.0002850582532119006,\n",
       " -0.05772387981414795,\n",
       " -0.01624906435608864,\n",
       " 0.013954561203718185,\n",
       " -0.05249323323369026,\n",
       " -0.038979943841695786,\n",
       " 0.004357198253273964,\n",
       " 0.019828516989946365,\n",
       " -0.00764873344451189,\n",
       " -0.007084849756211042,\n",
       " 0.026615779846906662,\n",
       " -0.03134351968765259,\n",
       " 0.020781533792614937,\n",
       " 0.013537929393351078,\n",
       " -0.040345702320337296,\n",
       " 0.022295644506812096,\n",
       " -0.02045341208577156,\n",
       " -0.02416311763226986,\n",
       " -0.009915377013385296,\n",
       " 0.027242843061685562,\n",
       " -0.0160831268876791,\n",
       " 0.02750091254711151,\n",
       " 0.008965492248535156,\n",
       " 0.02615620754659176,\n",
       " 0.03669346496462822,\n",
       " 0.013042710721492767,\n",
       " 0.06410396099090576,\n",
       " -0.019636668264865875,\n",
       " -0.06589405238628387,\n",
       " 0.0375266894698143,\n",
       " -0.02013162150979042,\n",
       " 0.012452137656509876,\n",
       " 0.029401293024420738,\n",
       " 0.008281425572931767,\n",
       " 0.016759444028139114,\n",
       " -0.014283740893006325,\n",
       " -0.026384390890598297,\n",
       " 0.0446876622736454,\n",
       " -0.00241871178150177,\n",
       " -0.01122561376541853,\n",
       " -0.06186794862151146,\n",
       " 0.03421890735626221,\n",
       " -0.018947230651974678,\n",
       " 0.04518605023622513,\n",
       " -0.07771658897399902,\n",
       " -0.04835588485002518,\n",
       " 0.0031389507930725813,\n",
       " -0.004049859009683132,\n",
       " -0.06726452708244324,\n",
       " -0.015795869752764702,\n",
       " -0.04038846492767334,\n",
       " 0.013851397670805454,\n",
       " 0.018899574875831604,\n",
       " -0.015654779970645905,\n",
       " 0.04540354385972023,\n",
       " 0.05116920545697212,\n",
       " 0.03905745968222618,\n",
       " -0.060086220502853394,\n",
       " 0.004896475002169609,\n",
       " 0.005941033363342285,\n",
       " 0.005604639649391174,\n",
       " 0.05208662897348404,\n",
       " -0.008962966501712799,\n",
       " 0.04167116805911064,\n",
       " -0.03162044286727905,\n",
       " -0.011213001795113087,\n",
       " -0.002425567014142871,\n",
       " -0.0095229372382164,\n",
       " 0.05663498118519783,\n",
       " 0.05532152205705643,\n",
       " -0.014627614989876747,\n",
       " 0.028465839102864265,\n",
       " -0.026630250737071037,\n",
       " -0.015246368013322353,\n",
       " -0.02774091251194477,\n",
       " -0.06943566352128983,\n",
       " 0.04066603630781174,\n",
       " -0.02346319705247879,\n",
       " -0.039878811687231064,\n",
       " -0.011883178725838661,\n",
       " 0.06250286102294922,\n",
       " -0.039146434515714645,\n",
       " 0.015018929727375507,\n",
       " -0.017022378742694855,\n",
       " -0.0019147424027323723,\n",
       " 0.0325455404818058,\n",
       " 0.00486141350120306,\n",
       " -0.04899516701698303,\n",
       " -0.01992950215935707,\n",
       " 0.009707805700600147,\n",
       " 0.031302861869335175,\n",
       " -0.027589421719312668,\n",
       " -0.051157645881175995,\n",
       " -0.037111829966306686,\n",
       " 0.1155155748128891,\n",
       " -0.04011806100606918,\n",
       " -0.004767966456711292,\n",
       " 0.016112525016069412,\n",
       " 0.039499107748270035,\n",
       " -0.022786540910601616,\n",
       " -0.006497627589851618,\n",
       " -0.06417474895715714,\n",
       " -0.022740919142961502,\n",
       " -0.025913625955581665,\n",
       " -0.05079182982444763,\n",
       " -0.015801966190338135,\n",
       " 0.055325333029031754,\n",
       " -0.011784359812736511,\n",
       " 0.006525163538753986,\n",
       " 0.009593199007213116,\n",
       " 0.0008535044617019594,\n",
       " -0.021076394245028496,\n",
       " -0.06920819729566574,\n",
       " -0.01764371059834957,\n",
       " 0.02779540605843067,\n",
       " -0.017467226833105087,\n",
       " -0.05160794407129288,\n",
       " -0.00955461896955967,\n",
       " -0.022385695949196815,\n",
       " -0.021353481337428093,\n",
       " -0.05032837763428688,\n",
       " -0.006860449444502592,\n",
       " 0.04655497893691063,\n",
       " -0.026929756626486778,\n",
       " 0.01794210821390152,\n",
       " -0.02923516184091568,\n",
       " -0.0332653671503067,\n",
       " -0.009896914474666119,\n",
       " 0.02028190903365612,\n",
       " 0.011755862273275852,\n",
       " -0.034854743629693985,\n",
       " -0.014305639080703259,\n",
       " 0.058481376618146896,\n",
       " 0.027386711910367012,\n",
       " -0.011304806917905807,\n",
       " 0.015282242558896542,\n",
       " 0.012149326503276825,\n",
       " -0.053166478872299194,\n",
       " 0.0197703056037426,\n",
       " 0.0275979433208704,\n",
       " -0.05886092409491539,\n",
       " 0.009243549779057503,\n",
       " 0.03269265219569206,\n",
       " -5.1387989515205845e-05,\n",
       " 0.03457881510257721,\n",
       " -0.004284382797777653,\n",
       " -0.015897858887910843,\n",
       " -0.006908179726451635,\n",
       " -0.025552846491336823,\n",
       " -0.023994576185941696,\n",
       " -0.05002641677856445,\n",
       " -0.03335259482264519,\n",
       " -0.05943126603960991,\n",
       " -0.026151839643716812,\n",
       " -0.08908814936876297,\n",
       " 0.019589468836784363,\n",
       " 0.04145211726427078,\n",
       " 0.006475525442510843,\n",
       " -0.07025504112243652,\n",
       " 0.019316192716360092,\n",
       " -0.014481930993497372,\n",
       " 0.013423518277704716,\n",
       " 0.029660075902938843,\n",
       " 8.469551539747044e-05,\n",
       " -0.006764828693121672,\n",
       " 0.07324924319982529,\n",
       " 0.026628538966178894,\n",
       " -0.03251069039106369,\n",
       " 0.03791547194123268,\n",
       " 0.005101160611957312,\n",
       " -0.005624540150165558,\n",
       " -0.04301062598824501,\n",
       " 0.027128765359520912,\n",
       " -0.028572380542755127,\n",
       " -0.0476725697517395,\n",
       " 0.026210876181721687,\n",
       " 0.08166244626045227,\n",
       " 0.10125695168972015,\n",
       " 0.04269399866461754,\n",
       " 0.03399597480893135,\n",
       " 0.010209919884800911,\n",
       " -0.00048557037371210754,\n",
       " -0.004522261209785938,\n",
       " 0.010201063007116318,\n",
       " 0.057933706790208817,\n",
       " 0.06220415607094765,\n",
       " -0.0054861861281096935,\n",
       " -0.05150578171014786,\n",
       " 0.0021591580007225275,\n",
       " 0.056812576949596405,\n",
       " 0.0612584687769413,\n",
       " 0.015490422025322914,\n",
       " -0.02181972935795784,\n",
       " -0.038188185542821884,\n",
       " 0.027472885325551033,\n",
       " -0.015947788953781128,\n",
       " -0.027241045609116554,\n",
       " 0.03186565637588501,\n",
       " -0.008822263218462467,\n",
       " -0.02542799897491932,\n",
       " -0.01327599585056305,\n",
       " 0.009440495632588863,\n",
       " -0.017917662858963013,\n",
       " -0.02301339991390705,\n",
       " 0.019735505804419518,\n",
       " 0.022749092429876328,\n",
       " -0.008465288206934929,\n",
       " 0.05198923125863075,\n",
       " 0.03610198572278023,\n",
       " 0.005524099338799715,\n",
       " -0.007259771227836609,\n",
       " -0.023630041629076004,\n",
       " 0.011898258700966835,\n",
       " 0.014683130197227001,\n",
       " 0.006605922244489193,\n",
       " 0.018918242305517197,\n",
       " 0.033086471259593964,\n",
       " 0.009352795779705048,\n",
       " 0.02538147009909153,\n",
       " 0.0017331556882709265,\n",
       " -0.04278094694018364,\n",
       " 0.0028899703174829483,\n",
       " -0.022017942741513252,\n",
       " 0.007207790389657021,\n",
       " 0.0028471092227846384,\n",
       " 0.002696448238566518,\n",
       " -0.03438250347971916,\n",
       " -0.0510806143283844,\n",
       " -0.034517280757427216,\n",
       " 0.10025496780872345,\n",
       " -0.07281593978404999,\n",
       " 0.007407755590975285,\n",
       " -0.0658956989645958,\n",
       " 0.03623507171869278,\n",
       " -0.017215963453054428,\n",
       " 0.023063337430357933,\n",
       " 0.023180311545729637,\n",
       " 0.03262705355882645,\n",
       " -0.05056038126349449,\n",
       " 0.02644210122525692,\n",
       " 0.09654156118631363,\n",
       " -0.03146858140826225,\n",
       " 0.009406603872776031,\n",
       " 0.006058161146938801,\n",
       " 0.0924575924873352,\n",
       " 0.005995325278490782,\n",
       " -0.0014445655979216099,\n",
       " 0.0018445805180817842,\n",
       " 0.04564281925559044,\n",
       " 0.0023293569684028625,\n",
       " -0.018168633803725243,\n",
       " 0.060181111097335815,\n",
       " 0.04872642457485199,\n",
       " 0.008104979991912842,\n",
       " 0.01362675428390503,\n",
       " 0.012416227720677853,\n",
       " 0.015856634825468063,\n",
       " 0.020654497668147087,\n",
       " 0.010801911354064941,\n",
       " 0.011183397844433784,\n",
       " -0.0112007362768054,\n",
       " 0.009495883248746395,\n",
       " -0.003336343914270401,\n",
       " -0.022465992718935013,\n",
       " -0.01152526494115591,\n",
       " -0.0012816016096621752,\n",
       " 0.022212427109479904,\n",
       " -0.008272911421954632,\n",
       " 0.0662248283624649,\n",
       " 0.0017409963766112924,\n",
       " 0.01935916766524315,\n",
       " 0.028920697048306465,\n",
       " 0.0050751883536577225,\n",
       " 0.008762583136558533,\n",
       " 0.004785844590514898,\n",
       " 0.004152222070842981,\n",
       " 0.017524363473057747,\n",
       " 0.02145509421825409,\n",
       " -0.009930456057190895,\n",
       " -0.04378340765833855,\n",
       " -0.0039490326307713985,\n",
       " 0.029694652184844017,\n",
       " 0.030016671866178513,\n",
       " 0.002066966611891985,\n",
       " -0.005986869800835848,\n",
       " 0.015527931042015553,\n",
       " -0.009455767460167408,\n",
       " -0.05929016321897507,\n",
       " 0.04148245230317116,\n",
       " -0.042255252599716187,\n",
       " 0.044870033860206604,\n",
       " 0.006883156485855579,\n",
       " -0.021884893998503685,\n",
       " 0.027032336220145226,\n",
       " 0.026221906766295433,\n",
       " -0.012020806781947613,\n",
       " -0.011858230456709862,\n",
       " 0.007319210097193718,\n",
       " -0.05155386030673981,\n",
       " -0.019934270530939102,\n",
       " 0.021964482963085175,\n",
       " -0.007208237424492836,\n",
       " -0.01611560955643654,\n",
       " -0.01832117699086666,\n",
       " -0.018534570932388306,\n",
       " 0.028255939483642578,\n",
       " -0.028604865074157715,\n",
       " 0.0045092348009347916,\n",
       " 0.06328833103179932,\n",
       " 0.0026684915646910667,\n",
       " 0.06935085356235504,\n",
       " 0.030285807326436043,\n",
       " -0.0072064632549881935,\n",
       " 0.04892483726143837,\n",
       " 0.04525882005691528,\n",
       " 0.011162920854985714,\n",
       " 0.005811761133372784,\n",
       " 0.020374305546283722,\n",
       " -0.022701021283864975,\n",
       " 0.05941203236579895,\n",
       " -0.08216503262519836,\n",
       " 0.03020727075636387,\n",
       " 0.08212774991989136,\n",
       " -0.0017191368388012052,\n",
       " 0.04218856245279312,\n",
       " 0.007955696433782578,\n",
       " 0.024574290961027145,\n",
       " -0.019159959629178047,\n",
       " 0.023165764287114143,\n",
       " 0.005339767783880234,\n",
       " -0.010721830651164055,\n",
       " -0.039889320731163025,\n",
       " -0.08704347908496857,\n",
       " 0.007089109160006046,\n",
       " 0.012682301923632622,\n",
       " -0.006281679030507803,\n",
       " 0.055093441158533096,\n",
       " 0.013368443585932255,\n",
       " 0.10058940201997757,\n",
       " -0.013442307710647583,\n",
       " -0.020061086863279343,\n",
       " -0.007948054000735283,\n",
       " -0.0245278961956501,\n",
       " 0.016831280663609505,\n",
       " -0.021172819659113884,\n",
       " 0.017827028408646584,\n",
       " 0.036265816539525986,\n",
       " -0.04229879379272461,\n",
       " -0.00725907227024436,\n",
       " 0.014222947880625725,\n",
       " 0.028224041685461998,\n",
       " -0.009996863082051277,\n",
       " -0.026122380048036575,\n",
       " -0.001095862127840519,\n",
       " -0.009134811349213123,\n",
       " -0.040405623614788055,\n",
       " -0.06609325110912323,\n",
       " 0.05744742974638939,\n",
       " -0.05214317515492439,\n",
       " -0.020790813490748405,\n",
       " 0.014703563414514065,\n",
       " -0.03375716134905815,\n",
       " -0.029894601553678513,\n",
       " -0.063018798828125,\n",
       " 0.03650415688753128,\n",
       " 0.025797823444008827,\n",
       " 0.019114984199404716,\n",
       " -0.0020520559046417475,\n",
       " -0.027659693732857704,\n",
       " 0.04201897233724594,\n",
       " 0.06972438097000122,\n",
       " -0.03552032262086868,\n",
       " -0.02683877944946289,\n",
       " 0.017722511664032936,\n",
       " 0.027583347633481026,\n",
       " -0.013112622313201427,\n",
       " -0.0072358050383627415,\n",
       " -0.009192616678774357,\n",
       " 0.03989480435848236,\n",
       " -0.002296261489391327,\n",
       " 0.007962706498801708,\n",
       " 0.06376756727695465,\n",
       " 0.06044555455446243,\n",
       " -0.05519396811723709,\n",
       " 0.046915676444768906,\n",
       " 0.04305407404899597,\n",
       " -0.0007578080403618515,\n",
       " 0.04777282103896141,\n",
       " 0.026165876537561417,\n",
       " -0.011348569765686989,\n",
       " 0.011348064057528973,\n",
       " -0.06600172072649002,\n",
       " 0.013850552961230278,\n",
       " -0.008699705824255943,\n",
       " 0.03232647106051445,\n",
       " -0.006745372898876667,\n",
       " 0.002460053889080882,\n",
       " -0.06557206064462662,\n",
       " -0.03465091064572334,\n",
       " -0.032419800758361816,\n",
       " 0.040477555245161057,\n",
       " 0.038267675787210464,\n",
       " 0.001803200226277113,\n",
       " 0.055773213505744934,\n",
       " -0.05181673541665077,\n",
       " 0.021067284047603607,\n",
       " 0.011340075172483921,\n",
       " -0.03807312250137329,\n",
       " 0.021218759939074516,\n",
       " 0.02404620312154293,\n",
       " -0.005858881399035454,\n",
       " -0.006984133739024401,\n",
       " -0.008188181556761265,\n",
       " 0.031457558274269104,\n",
       " 0.023899802938103676,\n",
       " 0.04827316105365753,\n",
       " 0.03659069910645485,\n",
       " -0.05923200398683548,\n",
       " -0.021913185715675354,\n",
       " 0.03386503458023071,\n",
       " 0.04563235118985176,\n",
       " -0.0015098156873136759,\n",
       " -0.026404526084661484,\n",
       " 0.0006306878058239818,\n",
       " 0.03731360286474228,\n",
       " -0.024228380993008614,\n",
       " 0.009018109180033207,\n",
       " 0.030946360900998116,\n",
       " 0.006658236030489206,\n",
       " -0.007981893606483936,\n",
       " 0.01904856599867344,\n",
       " -0.022934440523386,\n",
       " 0.0296107679605484,\n",
       " -0.03155604377388954,\n",
       " -0.012304776348173618,\n",
       " 0.0006550308899022639,\n",
       " -0.012464601546525955,\n",
       " 0.03594944253563881,\n",
       " -0.0705164298415184,\n",
       " -0.0040242308750748634,\n",
       " 0.07100523263216019,\n",
       " 0.04208529368042946,\n",
       " -0.08130843192338943,\n",
       " -0.09534409642219543,\n",
       " 0.008643563836812973,\n",
       " -0.07183621823787689,\n",
       " 0.06156414374709129,\n",
       " -0.07089749723672867,\n",
       " -0.017678307369351387,\n",
       " -0.008978075347840786,\n",
       " 0.030645770952105522,\n",
       " -0.05911853536963463,\n",
       " -0.06542827934026718,\n",
       " -0.0012679985957220197,\n",
       " -0.046609606593847275,\n",
       " -0.03188499063253403,\n",
       " 0.025667913258075714,\n",
       " -0.03192581608891487,\n",
       " 0.05244556441903114,\n",
       " 0.0002880618558265269,\n",
       " 0.01899315044283867,\n",
       " 0.0007680505514144897,\n",
       " 0.005816693417727947,\n",
       " -0.02471320331096649,\n",
       " 0.00049072370165959,\n",
       " -0.0294587891548872,\n",
       " -0.022769466042518616,\n",
       " -0.029076142236590385,\n",
       " -0.006863587070256472,\n",
       " 0.048787686973810196,\n",
       " 0.016221502795815468,\n",
       " 0.023360487073659897,\n",
       " 0.017650175839662552,\n",
       " -0.00790015421807766,\n",
       " -0.0041053700260818005,\n",
       " -0.017728829756379128,\n",
       " 0.005146570038050413,\n",
       " -0.03272509202361107,\n",
       " 0.07436607778072357,\n",
       " 0.010954830795526505,\n",
       " -0.00919382181018591,\n",
       " 0.011030493304133415,\n",
       " -0.0785832554101944,\n",
       " 0.08313117921352386,\n",
       " -0.004498304799199104,\n",
       " -0.028876760974526405,\n",
       " 0.004015831742435694,\n",
       " 0.00951590295881033,\n",
       " -0.017185650765895844,\n",
       " 0.08497815579175949,\n",
       " -0.017817968502640724,\n",
       " 0.01712643913924694,\n",
       " 0.037755049765110016,\n",
       " 0.00888756476342678,\n",
       " -0.02750120870769024,\n",
       " -0.010206486098468304,\n",
       " 0.004310502205044031,\n",
       " -0.0342852883040905,\n",
       " -0.06618905812501907,\n",
       " -0.07213648408651352,\n",
       " -0.011562785133719444,\n",
       " 0.01601698063313961,\n",
       " -0.09259406477212906,\n",
       " 0.013123595155775547,\n",
       " -0.04861074313521385,\n",
       " -0.0062404233030974865,\n",
       " 0.043690264225006104,\n",
       " 0.031744856387376785,\n",
       " -0.013682636432349682,\n",
       " 0.02134283073246479,\n",
       " -0.02698885276913643,\n",
       " 0.014090115204453468,\n",
       " 0.007817617617547512,\n",
       " 0.04092102497816086,\n",
       " 0.009999734349548817,\n",
       " 0.0041904812678694725,\n",
       " -0.030037447810173035,\n",
       " -0.0441267304122448,\n",
       " 0.06384799629449844,\n",
       " -0.020861851051449776,\n",
       " 0.06686430424451828,\n",
       " 0.02203928306698799,\n",
       " -0.007886246778070927,\n",
       " 0.012559964321553707,\n",
       " 0.014668947085738182,\n",
       " 0.011578661389648914,\n",
       " -0.026714621111750603,\n",
       " 0.03288481384515762,\n",
       " 0.013203495182096958,\n",
       " 0.0038635896053165197,\n",
       " 0.0039313496090471745,\n",
       " 0.001874922076240182,\n",
       " 0.0017691438551992178,\n",
       " 0.06823508441448212,\n",
       " -0.030270494520664215,\n",
       " 0.05959996208548546,\n",
       " -0.014263954944908619,\n",
       " 0.007758581079542637,\n",
       " -0.0009683708776719868,\n",
       " 0.0033778243232518435,\n",
       " -0.03782501444220543,\n",
       " 0.04262348264455795,\n",
       " 0.034219928085803986,\n",
       " 0.01696016639471054,\n",
       " 0.0841711163520813,\n",
       " -0.03928273543715477,\n",
       " -0.06223518028855324,\n",
       " -0.003479637671262026,\n",
       " -0.03386871516704559,\n",
       " 0.014342541806399822,\n",
       " 0.02156110294163227,\n",
       " 0.0020175862591713667,\n",
       " 0.013670770451426506,\n",
       " -0.021771958097815514,\n",
       " 0.01693614386022091,\n",
       " -0.033426959067583084,\n",
       " -0.0301163662225008,\n",
       " 0.0472169890999794,\n",
       " -0.027364186942577362,\n",
       " -0.022358793765306473,\n",
       " 0.004493711516261101,\n",
       " 0.01790003292262554,\n",
       " 0.07272481173276901,\n",
       " 0.005247009918093681,\n",
       " -0.07685832679271698,\n",
       " -0.003916303161531687,\n",
       " -0.02872679941356182,\n",
       " 0.0033624155912548304,\n",
       " 0.02600654773414135,\n",
       " 0.001129772630520165,\n",
       " -0.028148170560598373,\n",
       " 0.007218290586024523,\n",
       " 0.002530645113438368,\n",
       " 0.031115401536226273,\n",
       " -0.03764040768146515,\n",
       " -0.010936916805803776,\n",
       " -0.08124617487192154,\n",
       " -0.03979291021823883,\n",
       " 0.011936932802200317,\n",
       " -0.009141847491264343,\n",
       " -0.016471853479743004,\n",
       " -0.06177320331335068,\n",
       " 0.03147303685545921,\n",
       " -0.028461644425988197,\n",
       " 0.013967383652925491,\n",
       " 0.03842620924115181,\n",
       " -0.03636989742517471,\n",
       " -0.005116496700793505,\n",
       " -0.025941716507077217,\n",
       " -0.009060063399374485,\n",
       " 0.018031232059001923,\n",
       " -0.03604673221707344,\n",
       " 0.01739196665585041,\n",
       " -0.05340130999684334,\n",
       " -0.01800360530614853,\n",
       " 0.055991023778915405,\n",
       " -0.033284567296504974,\n",
       " -0.03622598573565483,\n",
       " -0.056331947445869446,\n",
       " 0.005739946383982897,\n",
       " -0.05082958936691284,\n",
       " -0.027943100780248642,\n",
       " 0.004812782164663076,\n",
       " -0.020595954731106758,\n",
       " 0.004962252452969551,\n",
       " 0.06813400238752365]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.embed_query(\n",
    "    \"which talks are related to sports?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.\\nLearn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.\\n\\n## Description\\nIn the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\\r\\nThe presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Yannis MOUDERE\\nI am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\\n\\n\", metadata={'guid': '2c108787-14a0-5b3b-91df-80d011479465', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-54-enhancing-event-analysis-at-scale-leveraging-tracking-data-in-sports-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/BWC9DD/', 'title': 'Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Learn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.', 'description': 'In the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\\r\\nThe presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 68, 'code': 'BCNSWC', 'public_name': 'Yannis MOUDERE', 'biography': \"I am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Yannis MOUDERE\\nI am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\\n\"}),\n",
       " Document(page_content='# Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches\\nThrough single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.\\n\\n## Description\\nTennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n\\n', metadata={'guid': '795350ad-6bec-5f36-b011-fb242a977bef', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-57-computer-vision-at-the-dutch-tennis-federation-utilizing-yolo-to-create-insights-for-coaches', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/UGNXZY/', 'title': 'Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Through single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.', 'description': 'Tennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 40, 'code': 'VRCBRB', 'public_name': 'Max Brouwer', 'biography': 'Max works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.', 'answers': []}, {'id': 70, 'code': 'ATL7DQ', 'public_name': 'Matthias Kempe', 'biography': 'Dr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n'}),\n",
       " Document(page_content=\"# Predicting the Spring Classics of cycling with my first neural network\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\n\\n## Description\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\\n\", metadata={'guid': '03ccd5af-7a72-58de-acbf-200dc5891883', 'logo': '', 'date': Timestamp('2024-07-11 12:00:00+0200', tz='UTC+02:00'), 'start': '12:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-25-predicting-the-spring-classics-of-cycling-with-my-first-neural-network', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/7A7YQC/', 'title': 'Predicting the Spring Classics of cycling with my first neural network', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.', 'description': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 34, 'code': '9WUZE9', 'public_name': 'Rob Claessens', 'biography': \"I'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\"}),\n",
       " Document(page_content=\"# How I lost 1000€ betting on CS:GO with machine learning and Python\\nPeople have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\\n\\n## Description\\nThis presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\\n\", metadata={'guid': 'aa661e50-ef5d-5d3a-8b84-2d798207eca0', 'logo': '', 'date': Timestamp('2024-07-11 16:05:00+0200', tz='UTC+02:00'), 'start': '16:05', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-31-how-i-lost-1000-betting-on-cs-go-with-machine-learning-and-python', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/WP7S3A/', 'title': 'How I lost 1000€ betting on CS:GO with machine learning and Python', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"People have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\", 'description': \"This presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 42, 'code': 'D7YTNK', 'public_name': 'Pedro Tabacof', 'biography': \"Pedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\"})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"which talks are related to sports?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.\n",
      "Learn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.\n",
      "\n",
      "## Description\n",
      "In the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\n",
      "The presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.\n",
      "\n",
      "## Timeslot\n",
      "2024-07-11T14:45:00+02:00 with a duration of 00:30\n",
      "\n",
      "## Room\n",
      "If (1.1)\n",
      "\n",
      "## Speaker\n",
      " ### Yannis MOUDERE\n",
      "I am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\n",
      "At the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    retriever.invoke(\n",
    "        \"which talks are related to sports?\"\n",
    "    )[0].page_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a list of talks related to sports:\n",
       "\n",
       "- Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.\n",
       "- Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches.\n",
       "- Predicting the Spring Classics of cycling with my first neural network.\n",
       "- How I lost 1000€ betting on CS:GO with machine learning and Python. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(rag_chain.invoke(\n",
    "    \"which talks are related to sports? return a bullet list\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Please provide me with more context about \"RAG talk.\"  The acronym \"RAG\" can stand for many things, and without more information, I can't tell you who hosts it, where it is, or when it happens. \n",
       "\n",
       "For example, are you referring to:\n",
       "\n",
       "* **A talk about a specific topic that includes the acronym \"RAG\"?**  (If so, what is the topic?)\n",
       "* **A talk hosted by an organization with \"RAG\" in the name?** (If so, what is the organization?)\n",
       "* **Something else entirely?**\n",
       "\n",
       "Please give me more details so I can help you find the information you need! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\n",
    "    llm.invoke(\n",
    "        \"what is the RAG talk about? who hosts it? where and when can i go to see it?\"\n",
    "    ).content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The RAG talk is about Retrieval Augmented Generation, a technique used in AI to enhance Large Language Models. It is hosted by Jeroen Overschie, a Machine Learning Engineer at Xebia Data. You can attend the talk on July 11th, 2024, at 11:15 AM in the Else (1.3) room. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(rag_chain.invoke(\n",
    "    \"what is the RAG talk about? who hosts it? where and when can i go to see it?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_collection = [\n",
    "#     \"cat\",\n",
    "#     \"red cat\",\n",
    "#     \"white cat with funny nose\",\n",
    "#     \"dog\",\n",
    "#     \"pydata\",\n",
    "#     \"eindhoven\",\n",
    "#     \"python\",\n",
    "#     \"rust\",\n",
    "#     \"julia\",\n",
    "#     \"programming\",\n",
    "#     \"lion\",\n",
    "# ]\n",
    "# word_embeddings = embedding.embed_documents(word_collection)\n",
    "\n",
    "# # t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=len(word_collection) - 1, max_iter=25000)\n",
    "# word_embeddings_2d = tsne.fit_transform(\n",
    "#     np.array(word_embeddings)\n",
    "# )\n",
    "\n",
    "# # plot\n",
    "# word_embeddings_df = pd.DataFrame(word_embeddings_2d, columns=[\"x\", \"y\"])\n",
    "# word_embeddings_df[\"word\"] = word_collection\n",
    "# fig = px.scatter(word_embeddings_df, x=\"x\", y=\"y\", text=\"word\")\n",
    "# fig.update_traces(textposition='top center')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With chunking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_chunked = FAISS.load_local(\"data/basic_rag_vectorstore_chunked.faiss\", embeddings=embedding, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to reproduce vectorstore\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "# vectorstore_chunked = FAISS.from_documents(splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save\n",
    "# vectorstore_chunked.save_local(\"data/basic_rag_vectorstore_chunked.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chunked = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_chunked = (\n",
    "    {\"context\": retriever_chunked | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Maximizing marketplace experimentation: switchback design for small samples and subtle effects\\nConventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.\\n\\n## Description\\nIn this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing\\'s limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic\\'s relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They\\'ll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n\\n', metadata={'guid': '44976e6d-e92c-5a5f-94cd-e32b6c798e5e', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-21-maximizing-marketplace-experimentation-switchback-design-for-small-samples-and-subtle-effects', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/KBYKXY/', 'title': 'Maximizing marketplace experimentation: switchback design for small samples and subtle effects', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.', 'description': \"In this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic's relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They'll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 32, 'code': 'GFKEBB', 'public_name': 'Nazli M. Alagoz', 'biography': 'Naz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.', 'answers': []}, {'id': 86, 'code': 'EL3UZS', 'public_name': 'Joël Gastelaars', 'biography': 'Working on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n'}),\n",
       " Document(page_content=\"# How I lost 1000€ betting on CS:GO with machine learning and Python\\nPeople have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\\n\\n## Description\\nThis presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\\n\", metadata={'guid': 'aa661e50-ef5d-5d3a-8b84-2d798207eca0', 'logo': '', 'date': Timestamp('2024-07-11 16:05:00+0200', tz='UTC+02:00'), 'start': '16:05', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-31-how-i-lost-1000-betting-on-cs-go-with-machine-learning-and-python', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/WP7S3A/', 'title': 'How I lost 1000€ betting on CS:GO with machine learning and Python', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"People have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\", 'description': \"This presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 42, 'code': 'D7YTNK', 'public_name': 'Pedro Tabacof', 'biography': \"Pedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\"}),\n",
       " Document(page_content=\"# Evaluating LLM Frameworks\\nLarge Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\\n\\n## Description\\nAt CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\\n\", metadata={'guid': 'b883c44e-ef8a-5f2f-9fd8-2cb8e958d4a7', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-32-evaluating-llm-frameworks', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/33M979/', 'title': 'Evaluating LLM Frameworks', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Large Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\", 'description': \"At CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 43, 'code': 'LKUKVW', 'public_name': 'Ennia Suijkerbuijk', 'biography': \"Ennia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\"}),\n",
       " Document(page_content=\"# Scikit-Learn can do THAT?!\\nMany of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\\n\\n## Description\\nThere may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\\n\", metadata={'guid': 'ab2ef9d4-8af9-54d7-a3fe-272999159d7b', 'logo': '', 'date': Timestamp('2024-07-11 12:00:00+0200', tz='UTC+02:00'), 'start': '12:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-9-scikit-learn-can-do-that-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/3BF9AT/', 'title': 'Scikit-Learn can do THAT?!', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Many of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\", 'description': \"There may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 15, 'code': 'G7KRFK', 'public_name': 'Vincent D. Warmerdam', 'biography': \"Vincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\"})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\n",
    "    query=\"split testing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Maximizing marketplace experimentation: switchback design for small samples and subtle effects\\nConventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.\\n\\n## Description\\nIn this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing\\'s limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic\\'s relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They\\'ll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n\\n', metadata={'guid': '44976e6d-e92c-5a5f-94cd-e32b6c798e5e', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-21-maximizing-marketplace-experimentation-switchback-design-for-small-samples-and-subtle-effects', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/KBYKXY/', 'title': 'Maximizing marketplace experimentation: switchback design for small samples and subtle effects', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.', 'description': \"In this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic's relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They'll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 32, 'code': 'GFKEBB', 'public_name': 'Nazli M. Alagoz', 'biography': 'Naz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.', 'answers': []}, {'id': 86, 'code': 'EL3UZS', 'public_name': 'Joël Gastelaars', 'biography': 'Working on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n'}),\n",
       " Document(page_content=\"# Evaluating LLM Frameworks\\nLarge Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\\n\\n## Description\\nAt CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\\n\", metadata={'guid': 'b883c44e-ef8a-5f2f-9fd8-2cb8e958d4a7', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-32-evaluating-llm-frameworks', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/33M979/', 'title': 'Evaluating LLM Frameworks', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Large Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\", 'description': \"At CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 43, 'code': 'LKUKVW', 'public_name': 'Ennia Suijkerbuijk', 'biography': \"Ennia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\"}),\n",
       " Document(page_content=\"# How I lost 1000€ betting on CS:GO with machine learning and Python\\nPeople have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\\n\\n## Description\\nThis presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\\n\", metadata={'guid': 'aa661e50-ef5d-5d3a-8b84-2d798207eca0', 'logo': '', 'date': Timestamp('2024-07-11 16:05:00+0200', tz='UTC+02:00'), 'start': '16:05', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-31-how-i-lost-1000-betting-on-cs-go-with-machine-learning-and-python', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/WP7S3A/', 'title': 'How I lost 1000€ betting on CS:GO with machine learning and Python', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"People have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\", 'description': \"This presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 42, 'code': 'D7YTNK', 'public_name': 'Pedro Tabacof', 'biography': \"Pedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\"}),\n",
       " Document(page_content='# Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches\\nThrough single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.\\n\\n## Description\\nTennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n\\n', metadata={'guid': '795350ad-6bec-5f36-b011-fb242a977bef', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-57-computer-vision-at-the-dutch-tennis-federation-utilizing-yolo-to-create-insights-for-coaches', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/UGNXZY/', 'title': 'Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Through single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.', 'description': 'Tennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 40, 'code': 'VRCBRB', 'public_name': 'Max Brouwer', 'biography': 'Max works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.', 'answers': []}, {'id': 70, 'code': 'ATL7DQ', 'public_name': 'Matthias Kempe', 'biography': 'Dr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n'})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\n",
    "    query=\"a methodology for comparing two versions of a webpage or app against each other to determine which one performs better\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='and recognize its advantages over conventional A/B testing for more informed decisions and improved', metadata={'guid': '44976e6d-e92c-5a5f-94cd-e32b6c798e5e', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-21-maximizing-marketplace-experimentation-switchback-design-for-small-samples-and-subtle-effects', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/KBYKXY/', 'title': 'Maximizing marketplace experimentation: switchback design for small samples and subtle effects', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.', 'description': \"In this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic's relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They'll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 32, 'code': 'GFKEBB', 'public_name': 'Nazli M. Alagoz', 'biography': 'Naz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.', 'answers': []}, {'id': 86, 'code': 'EL3UZS', 'public_name': 'Joël Gastelaars', 'biography': 'Working on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n'}),\n",
       " Document(page_content='design as a practical alternative to conventional A/B testing. By addressing small sample size', metadata={'guid': '44976e6d-e92c-5a5f-94cd-e32b6c798e5e', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-21-maximizing-marketplace-experimentation-switchback-design-for-small-samples-and-subtle-effects', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/KBYKXY/', 'title': 'Maximizing marketplace experimentation: switchback design for small samples and subtle effects', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.', 'description': \"In this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic's relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They'll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 32, 'code': 'GFKEBB', 'public_name': 'Nazli M. Alagoz', 'biography': 'Naz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.', 'answers': []}, {'id': 86, 'code': 'EL3UZS', 'public_name': 'Joël Gastelaars', 'biography': 'Working on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n'}),\n",
       " Document(page_content=\"It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted\", metadata={'guid': '44976e6d-e92c-5a5f-94cd-e32b6c798e5e', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-21-maximizing-marketplace-experimentation-switchback-design-for-small-samples-and-subtle-effects', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/KBYKXY/', 'title': 'Maximizing marketplace experimentation: switchback design for small samples and subtle effects', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.', 'description': \"In this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic's relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They'll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 32, 'code': 'GFKEBB', 'public_name': 'Nazli M. Alagoz', 'biography': 'Naz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.', 'answers': []}, {'id': 86, 'code': 'EL3UZS', 'public_name': 'Joël Gastelaars', 'biography': 'Working on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n'}),\n",
       " Document(page_content='Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and', metadata={'guid': '44976e6d-e92c-5a5f-94cd-e32b6c798e5e', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-21-maximizing-marketplace-experimentation-switchback-design-for-small-samples-and-subtle-effects', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/KBYKXY/', 'title': 'Maximizing marketplace experimentation: switchback design for small samples and subtle effects', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Conventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.', 'description': \"In this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing's limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic's relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They'll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 32, 'code': 'GFKEBB', 'public_name': 'Nazli M. Alagoz', 'biography': 'Naz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.', 'answers': []}, {'id': 86, 'code': 'EL3UZS', 'public_name': 'Joël Gastelaars', 'biography': 'Working on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Joël Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n'})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_chunked.similarity_search(\n",
    "    query=\"a methodology for comparing two versions of a webpage or app against each other to determine which one performs better\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_chunked.similarity_search(\n",
    "    \"split testing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chunked.invoke(\n",
    "    \"split testing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chunked.invoke(\n",
    "    \"a methodology for comparing two versions of a webpage or app against each other to determine which one performs better\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case for Hybrid (keyword search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches\\nThrough single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.\\n\\n## Description\\nTennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n\\n', metadata={'guid': '795350ad-6bec-5f36-b011-fb242a977bef', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-57-computer-vision-at-the-dutch-tennis-federation-utilizing-yolo-to-create-insights-for-coaches', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/UGNXZY/', 'title': 'Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Through single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.', 'description': 'Tennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an “off-the-shelf” YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 40, 'code': 'VRCBRB', 'public_name': 'Max Brouwer', 'biography': 'Max works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.', 'answers': []}, {'id': 70, 'code': 'ATL7DQ', 'public_name': 'Matthias Kempe', 'biography': 'Dr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n'}),\n",
       " Document(page_content=\"# The Levels of RAG 🦜\\nLLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\\n\\n## Description\\nRetrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you 🫵:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk 📌\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. … so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community 🫂\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ❤️ Open Source Software\\r\\nRAG and LLM’s are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\\n\\n## Timeslot\\n2024-07-11T11:15:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n\\n\", metadata={'guid': 'cdc3ef0a-718c-5330-b7cf-a8bf3db2a3f6', 'logo': '/media/cfp/submissions/G8VWGY/Screenshot_2024-06-29_at_17.07.32_W5To5r0.png', 'date': Timestamp('2024-07-11 11:15:00+0200', tz='UTC+02:00'), 'start': '11:15', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-69-the-levels-of-rag-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/G8VWGY/', 'title': 'The Levels of RAG 🦜', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"LLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\", 'description': \"Retrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you 🫵:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk 📌\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. … so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community 🫂\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ❤️ Open Source Software\\r\\nRAG and LLM’s are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 51, 'code': 'YM9JHZ', 'public_name': 'Jeroen Overschie', 'biography': 'Jeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n'}),\n",
       " Document(page_content=\"# Evaluating LLM Frameworks\\nLarge Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\\n\\n## Description\\nAt CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\\n\", metadata={'guid': 'b883c44e-ef8a-5f2f-9fd8-2cb8e958d4a7', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-32-evaluating-llm-frameworks', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/33M979/', 'title': 'Evaluating LLM Frameworks', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Large Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\", 'description': \"At CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 43, 'code': 'LKUKVW', 'public_name': 'Ennia Suijkerbuijk', 'biography': \"Ennia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\"}),\n",
       " Document(page_content='# 🌳 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery 🛰️\\nA case study of how we use Deep Learning based photogrammetry to calculate the height of trees from very high resolution satellite imagery. We show the substantial improvement achieved by switching from classical photogrammetric techniques to a deep learning based model (implemented in PyTorch), and the challenges we had to overcome to make this solution work.\\n\\n## Description\\nThe risk that a tree poses to line infrastructure (such as power lines) is determined by several factors, chief among them the height of the particular tree. The increasing availability of very high resolution satellite imagery makes it possible to use photogrammetric techniques to extract height information from a set of stereo satellite images. By using satellite imagery we can achieve a scale not possible by manual measurement. \\r\\nWe found that classical techniques perform poorly on vegetation, and were handily outperformed by deep learning based techniques implemented in PyTorch. This improvement was not trivial to achieve however, as creating labelled data in sufficient quantity was quite challenging. By increasing the quality of our height predictions we were able to more accurately calculate risk for our customers.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Ferdinand Schenck\\nI am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.\\n\\n', metadata={'guid': 'b2343a84-726c-599a-8ef4-c1d3992a6ded', 'logo': '', 'date': Timestamp('2024-07-11 16:50:00+0200', tz='UTC+02:00'), 'start': '16:50', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-40--the-taller-the-tree-the-harder-the-fall-determining-tree-height-from-space-using-deep-learning-and-very-high-resolution-satellite-imagery-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/XXCFDM/', 'title': '🌳 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery 🛰️', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'A case study of how we use Deep Learning based photogrammetry to calculate the height of trees from very high resolution satellite imagery. We show the substantial improvement achieved by switching from classical photogrammetric techniques to a deep learning based model (implemented in PyTorch), and the challenges we had to overcome to make this solution work.', 'description': 'The risk that a tree poses to line infrastructure (such as power lines) is determined by several factors, chief among them the height of the particular tree. The increasing availability of very high resolution satellite imagery makes it possible to use photogrammetric techniques to extract height information from a set of stereo satellite images. By using satellite imagery we can achieve a scale not possible by manual measurement. \\r\\nWe found that classical techniques perform poorly on vegetation, and were handily outperformed by deep learning based techniques implemented in PyTorch. This improvement was not trivial to achieve however, as creating labelled data in sufficient quantity was quite challenging. By increasing the quality of our height predictions we were able to more accurately calculate risk for our customers.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 53, 'code': 'EJCLAA', 'public_name': 'Ferdinand Schenck', 'biography': 'I am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Ferdinand Schenck\\nI am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.\\n'})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"Maximizing \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# The Levels of RAG 🦜\\nLLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\\n\\n## Description\\nRetrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you 🫵:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk 📌\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. … so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community 🫂\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ❤️ Open Source Software\\r\\nRAG and LLM’s are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\\n\\n## Timeslot\\n2024-07-11T11:15:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n\\n\", metadata={'guid': 'cdc3ef0a-718c-5330-b7cf-a8bf3db2a3f6', 'logo': '/media/cfp/submissions/G8VWGY/Screenshot_2024-06-29_at_17.07.32_W5To5r0.png', 'date': Timestamp('2024-07-11 11:15:00+0200', tz='UTC+02:00'), 'start': '11:15', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-69-the-levels-of-rag-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/G8VWGY/', 'title': 'The Levels of RAG 🦜', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"LLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\", 'description': \"Retrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you 🫵:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk 📌\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. … so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community 🫂\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ❤️ Open Source Software\\r\\nRAG and LLM’s are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 51, 'code': 'YM9JHZ', 'public_name': 'Jeroen Overschie', 'biography': 'Jeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n'}),\n",
       " Document(page_content=\"# Scikit-Learn can do THAT?!\\nMany of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\\n\\n## Description\\nThere may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\\n\", metadata={'guid': 'ab2ef9d4-8af9-54d7-a3fe-272999159d7b', 'logo': '', 'date': Timestamp('2024-07-11 12:00:00+0200', tz='UTC+02:00'), 'start': '12:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-9-scikit-learn-can-do-that-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/3BF9AT/', 'title': 'Scikit-Learn can do THAT?!', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Many of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\", 'description': \"There may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 15, 'code': 'G7KRFK', 'public_name': 'Vincent D. Warmerdam', 'biography': \"Vincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I’m especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\"}),\n",
       " Document(page_content=\"# Evaluating LLM Frameworks\\nLarge Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\\n\\n## Description\\nAt CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\\n\", metadata={'guid': 'b883c44e-ef8a-5f2f-9fd8-2cb8e958d4a7', 'logo': '', 'date': Timestamp('2024-07-11 14:00:00+0200', tz='UTC+02:00'), 'start': '14:00', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-32-evaluating-llm-frameworks', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/33M979/', 'title': 'Evaluating LLM Frameworks', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"Large Language Models are everywhere these days. But how can you objectively evaluate whether a model or a prompt is performing properly? Let's dive into the world of LLM evaluation frameworks!\", 'description': \"At CM.com we have released a new GenAI product (in 2023, built in Python) which is currently used by over 50 clients in various countries. GenAI is a chatbot that leverages the power of LLMs, while protecting against their common pitfalls such as incorrectness & inappropriateness, by using a Retrieval Augmented Generation framework (RAG).\\r\\n\\r\\nAs newer & better models rapidly arise, and our clients continue to provide feedback on the product, our own product development cannot lag behind. But how do we know whether changing from e.g. ChatGPT to Gemini or Llama improves the replies for all conversations? And how can you do prompt optimization if you don't know what you're optimizing against?  To help us maintain our current chatbot quality while investigating other models/prompts, we have developed an evaluation framework in Python that can objectively evaluate several scenarios across a various of metrics. \\r\\n\\r\\nDuring these 30 minutes, I'll explain the principle behind RAG, highlight the huge development work being done in this field across the globe, give examples of several evaluation measures, and finally explain how we use these to move forward with our product. The talk is most interesting for Data Scientists and ML/AI/Prompt Engineers, but can be followed by anyone with some background knowledge on LLMs.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 43, 'code': 'LKUKVW', 'public_name': 'Ennia Suijkerbuijk', 'biography': \"Ennia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Ennia Suijkerbuijk\\nEnnia works as a Senior Data Scientist at CM.com. As part of the AI Tribe, she works on developing AI software solutions for companies across the globe. Her core focus nowadays lies with CM.com's state-of-the-art Generative AI Engine, which makes the power of LLMs & NLP available, easy to use and safe for companies in all sorts & sizes.\\n\"}),\n",
       " Document(page_content='# Software at ASML: the Force behind making microchips\\nIn a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. In addition, we discuss how ASML engages and cultivates Software talents. At the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future.\\n\\n## Description\\nA full description for this keynote will be announcIn a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. Key highlights include our innovative development environment, which fosters seamless integration and rapid iteration amongst teams. In addition, we discuss how ASML engages and cultivates Software talent through strategic initiatives and collaborative projects. By fostering a dynamic and innovative work environment, we empower our engineers to drive technological advancements and operational excellence.\\r\\n\\r\\nAt the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future of semiconductor fabrication.ed and added to the session when it becomes available.\\n\\n## Timeslot\\n2024-07-11T09:00:00+02:00 with a duration of 00:50\\n\\n## Room\\nREPL (2, mainstage)\\n\\n## Speaker\\n\\n', metadata={'guid': '654f9047-1ee1-57d0-95f7-220b16370422', 'logo': '', 'date': Timestamp('2024-07-11 09:00:00+0200', tz='UTC+02:00'), 'start': '09:00', 'duration': '00:50', 'room': 'REPL (2, mainstage)', 'slug': 'cfp-71-software-at-asml-the-force-behind-making-microchips', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/LKGBYR/', 'title': 'Software at ASML: the Force behind making microchips', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'In a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. In addition, we discuss how ASML engages and cultivates Software talents. At the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future.', 'description': 'A full description for this keynote will be announcIn a rapidly evolving landscape of semiconductor manufacturing, ASML’s advanced lithography technologies are pivotal in driving Moore’s Law forward. The focus of this talk is ASML’s holistic approach to Software development and acceleration of Software delivery to customers. Key highlights include our innovative development environment, which fosters seamless integration and rapid iteration amongst teams. In addition, we discuss how ASML engages and cultivates Software talent through strategic initiatives and collaborative projects. By fostering a dynamic and innovative work environment, we empower our engineers to drive technological advancements and operational excellence.\\r\\n\\r\\nAt the end of this presentation the attendees will have an understanding of ASML’s software ecosystem and its critical role in advancing the future of semiconductor fabrication.ed and added to the session when it becomes available.', 'recording_license': '', 'do_not_record': False, 'persons': [], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ''})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"What's the talk starting with Maximizing about?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The talk starting with \"Maximizing\" is about less common but noteworthy features of scikit-learn and its ecosystem.  It covers topics such as sparse datasets, larger-than-memory datasets, sample weight techniques, and more. The speaker aims to highlight these features and potentially live code some examples. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\n",
    "    rag_chain.invoke(\n",
    "        \"What's the talk starting with Maximizing about?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# Predicting the Spring Classics of cycling with my first neural network\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\n\\n## Description\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\\n\", metadata={'guid': '03ccd5af-7a72-58de-acbf-200dc5891883', 'logo': '', 'date': Timestamp('2024-07-11 12:00:00+0200', tz='UTC+02:00'), 'start': '12:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-25-predicting-the-spring-classics-of-cycling-with-my-first-neural-network', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/7A7YQC/', 'title': 'Predicting the Spring Classics of cycling with my first neural network', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.', 'description': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 34, 'code': '9WUZE9', 'public_name': 'Rob Claessens', 'biography': \"I'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\"}),\n",
       " Document(page_content='# Cloud? No Thanks! I’m Gonna Run GenAI on My AI PC\\nIn this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.\\n\\n## Description\\nIn this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master’s degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he’s a traveler. You can also talk with him about finance, especially investments.\\n\\n', metadata={'guid': 'bf13b897-74b8-5593-88c5-f547b655b25b', 'logo': '', 'date': Timestamp('2024-07-11 16:50:00+0200', tz='UTC+02:00'), 'start': '16:50', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-59-cloud-no-thanks-i-m-gonna-run-genai-on-my-ai-pc', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/SMX9CS/', 'title': 'Cloud? No Thanks! I’m Gonna Run GenAI on My AI PC', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'In this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.', 'description': 'In this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 74, 'code': 'JGCHB3', 'public_name': 'Dmitriy Pastushenkov', 'biography': 'Dmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master’s degree in Computer Science from Moscow Power Engineering Institute (Technical University).', 'answers': []}, {'id': 73, 'code': 'TAYW7K', 'public_name': 'Adrian Boguszewski', 'biography': 'AI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he’s a traveler. You can also talk with him about finance, especially investments.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master’s degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he’s a traveler. You can also talk with him about finance, especially investments.\\n'}),\n",
       " Document(page_content=\"# Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.\\nLearn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.\\n\\n## Description\\nIn the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\\r\\nThe presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Yannis MOUDERE\\nI am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\\n\\n\", metadata={'guid': '2c108787-14a0-5b3b-91df-80d011479465', 'logo': '', 'date': Timestamp('2024-07-11 14:45:00+0200', tz='UTC+02:00'), 'start': '14:45', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-54-enhancing-event-analysis-at-scale-leveraging-tracking-data-in-sports-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/BWC9DD/', 'title': 'Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Learn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.', 'description': 'In the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\\r\\nThe presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 68, 'code': 'BCNSWC', 'public_name': 'Yannis MOUDERE', 'biography': \"I am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Yannis MOUDERE\\nI am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\\n\"}),\n",
       " Document(page_content=\"# Sonic Pi - Live Coding as a tool for next-gen education.\\nSonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding.\\n\\n## Description\\nSonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding systems. We'll touch on concurrency, distributed programming, temporal logic, deterministic randomisation, event streams, hot swapping code and domain specific languages.\\r\\n\\r\\nGet ready for some serious live coded beats and a window into an exciting future of computing education.\\n\\n## Timeslot\\n2024-07-11T17:30:00+02:00 with a duration of 01:00\\n\\n## Room\\nREPL (2, mainstage)\\n\\n## Speaker\\n ### Sam Aaron\\nNone\\n\\n\", metadata={'guid': '4b443472-06fa-57a1-9682-a12461da68bf', 'logo': '', 'date': Timestamp('2024-07-11 17:30:00+0200', tz='UTC+02:00'), 'start': '17:30', 'duration': '01:00', 'room': 'REPL (2, mainstage)', 'slug': 'cfp-70-sonic-pi-live-coding-as-a-tool-for-next-gen-education-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/XYLXUP/', 'title': 'Sonic Pi - Live Coding as a tool for next-gen education.', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'Sonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding.', 'description': \"Sonic Pi is a free code-based music creation and performance tool that targets both education and professional musicians. It is possible for beginners to code fresh beats, driving bass lines and shimmering synth riffs. All this whilst teaching core computer science concepts such as sequencing, functions, variables, loops, data structures and algorithms.\\r\\n\\r\\nThis talk will briefly introduce Sonic Pi before taking a deep technical nose-dive into some of the interesting requirements of live coding systems. We'll touch on concurrency, distributed programming, temporal logic, deterministic randomisation, event streams, hot swapping code and domain specific languages.\\r\\n\\r\\nGet ready for some serious live coded beats and a window into an exciting future of computing education.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 87, 'code': 'TBD3DK', 'public_name': 'Sam Aaron', 'biography': None, 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Sam Aaron\\nNone\\n'})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"I'm gonna run \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Causal Forecasting: How to disentangle causal effects, while controlling for unobserved confounders and keeping accuracy\\nA lot of industry-available Machine Learning solutions for causal forecasting have a very particular blind spot: unobserved confounders. We will present an approach that allows you to combine state-of-the-art Machine Learning approaches with advanced Econometrics techniques to get the better of both worlds: accurate causal inference and good forecasting accuracy.\\n\\n## Description\\nCausal Forecasting is a very hot topic in the industry with many applications ranging from marketing spending to pricing. Disentangling causal effects from spurious correlations plays a key role when forecasts are used for decision making, such as in the case of pricing. Solutions available in the industry typically rely on Machine Learning methods that use techniques like DoubleML, Transformers, LSTM, and boosted tree algorithms. A common shortcoming of such solutions is that they do not account for the existence of unobserved confounders, such as world events, or other hard-to-measure effects that can bias the measurement of causal effects. We showcase a solution that was developed over the last 3 years that addresses these challenges by combining advanced Econometrics methods with  ML techniques. The case-study will focus on the example of retail pricing, but the solution is broadly applicable and it has been tested in different settings, including airline pricing.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Marc Nientker\\nMarc Nientker transitioned from a successful seven-year academic career in econometrics, where he contributed as a PhD and Assistant Professor, to the business world to apply his knowledge on a broader scale. He co-founded Acmetric, a strategic data science consultancy that focuses on transforming businesses through data-driven insights.\\r\\n\\r\\nAcmetric specializes in practical applications of econometrics in areas such as pricing, inventory optimization, product allocation, measurement, and more. His expertise supports organizations in understanding and implementing data-centric strategies that naturally lead to more informed decision-making and operational efficiencies.\\n\\n', metadata={'guid': '82194198-ad3c-5909-b0f5-5ad591a2b815', 'logo': '/media/cfp/submissions/8QNFLU/AC_Profile_picture_-_Marc_1_da8pCEz.jpg', 'date': Timestamp('2024-07-11 16:05:00+0200', tz='UTC+02:00'), 'start': '16:05', 'duration': '00:30', 'room': 'Else (1.3)', 'slug': 'cfp-55-causal-forecasting-how-to-disentangle-causal-effects-while-controlling-for-unobserved-confounders-and-keeping-accuracy', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/8QNFLU/', 'title': 'Causal Forecasting: How to disentangle causal effects, while controlling for unobserved confounders and keeping accuracy', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'A lot of industry-available Machine Learning solutions for causal forecasting have a very particular blind spot: unobserved confounders. We will present an approach that allows you to combine state-of-the-art Machine Learning approaches with advanced Econometrics techniques to get the better of both worlds: accurate causal inference and good forecasting accuracy.', 'description': 'Causal Forecasting is a very hot topic in the industry with many applications ranging from marketing spending to pricing. Disentangling causal effects from spurious correlations plays a key role when forecasts are used for decision making, such as in the case of pricing. Solutions available in the industry typically rely on Machine Learning methods that use techniques like DoubleML, Transformers, LSTM, and boosted tree algorithms. A common shortcoming of such solutions is that they do not account for the existence of unobserved confounders, such as world events, or other hard-to-measure effects that can bias the measurement of causal effects. We showcase a solution that was developed over the last 3 years that addresses these challenges by combining advanced Econometrics methods with  ML techniques. The case-study will focus on the example of retail pricing, but the solution is broadly applicable and it has been tested in different settings, including airline pricing.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 69, 'code': 'N733EQ', 'public_name': 'Marc Nientker', 'biography': 'Marc Nientker transitioned from a successful seven-year academic career in econometrics, where he contributed as a PhD and Assistant Professor, to the business world to apply his knowledge on a broader scale. He co-founded Acmetric, a strategic data science consultancy that focuses on transforming businesses through data-driven insights.\\r\\n\\r\\nAcmetric specializes in practical applications of econometrics in areas such as pricing, inventory optimization, product allocation, measurement, and more. His expertise supports organizations in understanding and implementing data-centric strategies that naturally lead to more informed decision-making and operational efficiencies.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Marc Nientker\\nMarc Nientker transitioned from a successful seven-year academic career in econometrics, where he contributed as a PhD and Assistant Professor, to the business world to apply his knowledge on a broader scale. He co-founded Acmetric, a strategic data science consultancy that focuses on transforming businesses through data-driven insights.\\r\\n\\r\\nAcmetric specializes in practical applications of econometrics in areas such as pricing, inventory optimization, product allocation, measurement, and more. His expertise supports organizations in understanding and implementing data-centric strategies that naturally lead to more informed decision-making and operational efficiencies.\\n'}),\n",
       " Document(page_content=\"# How I lost 1000€ betting on CS:GO with machine learning and Python\\nPeople have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\\n\\n## Description\\nThis presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\\n\", metadata={'guid': 'aa661e50-ef5d-5d3a-8b84-2d798207eca0', 'logo': '', 'date': Timestamp('2024-07-11 16:05:00+0200', tz='UTC+02:00'), 'start': '16:05', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-31-how-i-lost-1000-betting-on-cs-go-with-machine-learning-and-python', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/WP7S3A/', 'title': 'How I lost 1000€ betting on CS:GO with machine learning and Python', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': \"People have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\", 'description': \"This presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner’s curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\", 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 42, 'code': 'D7YTNK', 'public_name': 'Pedro Tabacof', 'biography': \"Pedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\"}),\n",
       " Document(page_content='# 🌳 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery 🛰️\\nA case study of how we use Deep Learning based photogrammetry to calculate the height of trees from very high resolution satellite imagery. We show the substantial improvement achieved by switching from classical photogrammetric techniques to a deep learning based model (implemented in PyTorch), and the challenges we had to overcome to make this solution work.\\n\\n## Description\\nThe risk that a tree poses to line infrastructure (such as power lines) is determined by several factors, chief among them the height of the particular tree. The increasing availability of very high resolution satellite imagery makes it possible to use photogrammetric techniques to extract height information from a set of stereo satellite images. By using satellite imagery we can achieve a scale not possible by manual measurement. \\r\\nWe found that classical techniques perform poorly on vegetation, and were handily outperformed by deep learning based techniques implemented in PyTorch. This improvement was not trivial to achieve however, as creating labelled data in sufficient quantity was quite challenging. By increasing the quality of our height predictions we were able to more accurately calculate risk for our customers.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Ferdinand Schenck\\nI am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.\\n\\n', metadata={'guid': 'b2343a84-726c-599a-8ef4-c1d3992a6ded', 'logo': '', 'date': Timestamp('2024-07-11 16:50:00+0200', tz='UTC+02:00'), 'start': '16:50', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-40--the-taller-the-tree-the-harder-the-fall-determining-tree-height-from-space-using-deep-learning-and-very-high-resolution-satellite-imagery-', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/XXCFDM/', 'title': '🌳 The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery 🛰️', 'subtitle': '', 'track': None, 'type': 'Talk', 'language': 'en', 'abstract': 'A case study of how we use Deep Learning based photogrammetry to calculate the height of trees from very high resolution satellite imagery. We show the substantial improvement achieved by switching from classical photogrammetric techniques to a deep learning based model (implemented in PyTorch), and the challenges we had to overcome to make this solution work.', 'description': 'The risk that a tree poses to line infrastructure (such as power lines) is determined by several factors, chief among them the height of the particular tree. The increasing availability of very high resolution satellite imagery makes it possible to use photogrammetric techniques to extract height information from a set of stereo satellite images. By using satellite imagery we can achieve a scale not possible by manual measurement. \\r\\nWe found that classical techniques perform poorly on vegetation, and were handily outperformed by deep learning based techniques implemented in PyTorch. This improvement was not trivial to achieve however, as creating labelled data in sufficient quantity was quite challenging. By increasing the quality of our height predictions we were able to more accurately calculate risk for our customers.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 53, 'code': 'EJCLAA', 'public_name': 'Ferdinand Schenck', 'biography': 'I am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.', 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': ' ### Ferdinand Schenck\\nI am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universität zu Berlin on the ATLAS experiment at CERN.\\n'}),\n",
       " Document(page_content=\"# Predicting the Spring Classics of cycling with my first neural network\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\n\\n## Description\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\\n\", metadata={'guid': '03ccd5af-7a72-58de-acbf-200dc5891883', 'logo': '', 'date': Timestamp('2024-07-11 12:00:00+0200', tz='UTC+02:00'), 'start': '12:00', 'duration': '00:30', 'room': 'If (1.1)', 'slug': 'cfp-25-predicting-the-spring-classics-of-cycling-with-my-first-neural-network', 'url': 'https://eindhoven2024.pydata.org/cfp/talk/7A7YQC/', 'title': 'Predicting the Spring Classics of cycling with my first neural network', 'subtitle': '', 'track': 'PySport - Sports Analytics', 'type': 'Talk', 'language': 'en', 'abstract': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.', 'description': 'Last year I attended PyData Eindhoven for the first time. I got inspired and now I’m back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I’m attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I’ll try to provide some insights into the model using SHAP values.', 'recording_license': '', 'do_not_record': False, 'persons': [{'id': 34, 'code': '9WUZE9', 'public_name': 'Rob Claessens', 'biography': \"I'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\", 'answers': []}], 'links': [], 'attachments': [], 'answers': [], 'persons_text': \" ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\"})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"predicting the\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown(\n",
    "#     format_docs(\n",
    "#         retriever.invoke(\"I'm gonna run\")\n",
    "#     )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levels-of-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
