{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_milvus.retrievers import MilvusCollectionHybridSearchRetriever\n",
    "from langchain_milvus.utils.sparse import BM25SparseEmbedding\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from pymilvus import (\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    FieldSchema,\n",
    "    RRFRanker,\n",
    "    connections,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "result = llm.invoke(\"hello!\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "len(embedding.embed_query(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = pd.read_json(\"data/pydata_eindhoven_2024_sessions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_URI = \"http://localhost:19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sessions[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9591461605237726,\n",
       " 1: 15.324768712979717,\n",
       " 5: 1.3499267169490157,\n",
       " 11: 0.4883314709749867,\n",
       " 12: 1.0216512475319814,\n",
       " 15: 1.7578579175523736,\n",
       " 16: 1.3499267169490157,\n",
       " 17: 0.4883314709749867,\n",
       " 20: 1.3499267169490157,\n",
       " 21: 1.0216512475319814,\n",
       " 23: 0.4727775561284613,\n",
       " 27: 0.4883314709749867,\n",
       " 31: 1.475197886261558,\n",
       " 42: 0.4883314709749867,\n",
       " 46: 0.9591461605237726,\n",
       " 47: 0.9766629419499734,\n",
       " 66: 0.0,\n",
       " 68: 0.0,\n",
       " 78: 1.3499267169490157,\n",
       " 79: 1.7578579175523736,\n",
       " 81: 1.4649944129249601,\n",
       " 82: 0.4883314709749867,\n",
       " 83: 0.4883314709749867,\n",
       " 84: 0.4883314709749867,\n",
       " 85: 0.4883314709749867,\n",
       " 92: 0.23638877806423064,\n",
       " 93: 0.4883314709749867,\n",
       " 94: 0.737598943130779,\n",
       " 95: 0.4883314709749867,\n",
       " 102: 0.9766629419499734,\n",
       " 104: 0.4795730802618863,\n",
       " 120: 1.3499267169490157,\n",
       " 121: 0.4795730802618863,\n",
       " 122: 0.0,\n",
       " 123: 6.129907485191888,\n",
       " 124: 7.006124747451109,\n",
       " 125: 1.7578579175523736,\n",
       " 126: 7.0314316702094946,\n",
       " 127: 1.9533258838999468,\n",
       " 128: 2.3353749158170363,\n",
       " 129: 1.7578579175523736,\n",
       " 130: 4.049780150847047,\n",
       " 131: 2.6998534338980313,\n",
       " 132: 0.9766629419499734,\n",
       " 133: 5.273573752657121,\n",
       " 134: 7.006124747451109,\n",
       " 135: 1.475197886261558,\n",
       " 136: 3.5157158351047473,\n",
       " 137: 0.4795730802618863,\n",
       " 138: 0.4795730802618863,\n",
       " 139: 2.3353749158170363,\n",
       " 140: 1.4387192407856588,\n",
       " 141: 1.0216512475319814,\n",
       " 142: 1.0216512475319814,\n",
       " 143: 2.6998534338980313,\n",
       " 144: 1.0216512475319814,\n",
       " 145: 1.0216512475319814,\n",
       " 146: 1.3499267169490157,\n",
       " 147: 0.737598943130779,\n",
       " 148: 2.3353749158170363,\n",
       " 149: 1.475197886261558,\n",
       " 150: 0.7091663341926919,\n",
       " 151: 1.7578579175523736,\n",
       " 152: 2.043302495063963,\n",
       " 153: 1.3499267169490157,\n",
       " 154: 0.4727775561284613,\n",
       " 155: 1.0216512475319814,\n",
       " 156: 1.7578579175523736,\n",
       " 157: 1.3499267169490157,\n",
       " 158: 1.7578579175523736,\n",
       " 159: 1.3499267169490157,\n",
       " 160: 1.7578579175523736,\n",
       " 161: 2.3353749158170363,\n",
       " 162: 2.3353749158170363,\n",
       " 163: 4.6707498316340725,\n",
       " 164: 2.3353749158170363,\n",
       " 165: 2.3353749158170363,\n",
       " 166: 1.3499267169490157,\n",
       " 167: 2.3353749158170363,\n",
       " 168: 1.3499267169490157,\n",
       " 169: 1.0216512475319814,\n",
       " 170: 2.3353749158170363,\n",
       " 171: 2.3353749158170363,\n",
       " 172: 1.0216512475319814,\n",
       " 173: 1.3499267169490157,\n",
       " 174: 0.737598943130779,\n",
       " 175: 0.737598943130779,\n",
       " 176: 1.7578579175523736,\n",
       " 177: 2.3353749158170363,\n",
       " 178: 2.3353749158170363,\n",
       " 179: 1.7578579175523736,\n",
       " 180: 1.0216512475319814,\n",
       " 181: 2.3353749158170363,\n",
       " 182: 1.7578579175523736,\n",
       " 183: 1.0216512475319814,\n",
       " 184: 2.3353749158170363,\n",
       " 185: 2.043302495063963,\n",
       " 187: 1.3499267169490157,\n",
       " 188: 1.7578579175523736,\n",
       " 189: 0.9591461605237726,\n",
       " 190: 0.0,\n",
       " 191: 1.0216512475319814,\n",
       " 192: 0.737598943130779,\n",
       " 193: 0.4795730802618863,\n",
       " 194: 0.737598943130779,\n",
       " 195: 2.3353749158170363,\n",
       " 196: 1.7578579175523736,\n",
       " 197: 1.7578579175523736,\n",
       " 198: 4.6707498316340725,\n",
       " 199: 2.3353749158170363,\n",
       " 200: 1.3499267169490157,\n",
       " 201: 2.3353749158170363,\n",
       " 202: 0.4883314709749867,\n",
       " 203: 0.4795730802618863,\n",
       " 204: 1.0216512475319814,\n",
       " 205: 0.4795730802618863,\n",
       " 206: 0.737598943130779,\n",
       " 207: 1.7578579175523736,\n",
       " 208: 2.3353749158170363,\n",
       " 209: 1.0216512475319814,\n",
       " 210: 2.3353749158170363,\n",
       " 211: 0.737598943130779,\n",
       " 212: 1.7578579175523736,\n",
       " 213: 2.3353749158170363,\n",
       " 214: 1.7578579175523736}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_embedding_func = BM25SparseEmbedding(corpus=texts)\n",
    "sparse_embedding_func.embed_query(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Initialize connection URI and establish connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(uri=CONNECTION_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define field names and their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_field = \"doc_id\"\n",
    "dense_field = \"dense_vector\"\n",
    "sparse_field = \"sparse_vector\"\n",
    "text_field = \"text\"\n",
    "fields = [\n",
    "    FieldSchema(\n",
    "        name=pk_field,\n",
    "        dtype=DataType.VARCHAR,\n",
    "        is_primary=True,\n",
    "        auto_id=True,\n",
    "        max_length=100,\n",
    "    ),\n",
    "    FieldSchema(name=dense_field, dtype=DataType.FLOAT_VECTOR, dim=768),\n",
    "    FieldSchema(name=sparse_field, dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "    FieldSchema(name=text_field, dtype=DataType.VARCHAR, max_length=65_535),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a collection with the defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = CollectionSchema(fields=fields, enable_dynamic_field=False)\n",
    "collection = Collection(\n",
    "    name=\"PyDataSessions1107\", schema=schema, consistency_level=\"Strong\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define index for dense and sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_index = {\"index_type\": \"FLAT\", \"metric_type\": \"IP\"}\n",
    "collection.create_index(\"dense_vector\", dense_index)\n",
    "sparse_index = {\"index_type\": \"SPARSE_INVERTED_INDEX\", \"metric_type\": \"IP\"}\n",
    "collection.create_index(\"sparse_vector\", sparse_index)\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert entities into the collection and load the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for text in texts:\n",
    "    entity = {\n",
    "        dense_field: embedding.embed_documents([text])[0],\n",
    "        sparse_field: sparse_embedding_func.embed_documents([text])[0],\n",
    "        text_field: text,\n",
    "    }\n",
    "    entities.append(entity)\n",
    "collection.insert(entities)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_search_params = {\"metric_type\": \"IP\"}\n",
    "dense_search_params = {\"metric_type\": \"IP\", \"params\": {}}\n",
    "retriever = MilvusCollectionHybridSearchRetriever(\n",
    "    collection=collection,\n",
    "    rerank=RRFRanker(k=60),\n",
    "    anns_fields=[dense_field, sparse_field],\n",
    "    field_embeddings=[embedding, sparse_embedding_func],\n",
    "    field_search_params=[dense_search_params, sparse_search_params],\n",
    "    top_k=4,\n",
    "    text_field=text_field,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# How I lost 1000‚Ç¨ betting on CS:GO with machine learning and Python\\nPeople have been using machine learning for sports betting for decades. Logistic regression applied to horse racing made someone a multi-millionaire in the 80s. While fun, betting is a losing proposition for most. The house always wins, right?\\r\\n\\r\\nWith a friend, I thought we could beat the house in e-sports by leveraging modern ML tools like LightGBM. E-sports betting is less sophisticated than football or horse racing i.e. the market is less efficient. There is a lot of online data and unknown teams. It was a space ripe for money-making, or so we thought.\\r\\n\\r\\nFirst, I will explain the theory behind e-sports betting with ML: what is an edge, financial decision-making, the expected value and decision rule for one bet, multiple bets with the Kelly criterion, probability calibration and the winner's curse.\\r\\n\\r\\nThen, I will explain how we built a web scraper to extract features, developed a probabilistic classifier using LightGBM, defined betting rules using the Kelly criterion, backtested it with a positive ROI, and then lost actual money, with many priceless lessons coming out of it.\\n\\n## Description\\nThis presentation goes in-depth on how to use ML for e-sports betting and the pitfalls one might fall in. More broadly, I try to connect ML with financial decision-making, which can be applied in other domains too (credit, fraud, marketing), targeting data scientists and ML practitioners who are interested in financial applications.\\r\\n\\r\\nFinancial decision-making is not just about being right (predictive modelling) but also about acting rightly (betting/trading strategy). To act correctly, one must understand concepts such as an edge, expected value/profits, probability calibration, winner's curse (selection bias), and so on. More importantly, any trading or betting strategy needs to be thoroughly validated with backtests and paper-trades and the risk and profitability quantified. My aim is to cover some of those important foundational topics, while providing pointers for further studies.\\r\\n\\r\\nThe presentation is divided into two parts: \\r\\n\\r\\n1. Foundations of ML applied to betting (15 min)\\r\\n   * What is your edge?\\r\\n   * Financial decision-making with ML\\r\\n   * One bet: Expected profits and decision rule\\r\\n   * Multiple bets: The Kelly criterion\\r\\n   * Probability calibration\\r\\n   * Winner‚Äôs curse\\r\\n2. CS:GO betting (10 min)\\r\\n   * Data scraping\\r\\n   * Feature engineering\\r\\n   * TrueSkill (with a side note on inferential vs predictive models)\\r\\n   * Modelling\\r\\n   * Evaluation\\r\\n   * Backtesting\\r\\n   * Why I lost 1000 euros\\r\\n\\r\\nThat is, I will present both the theory and practice, using my own failure as an illustrative example for the lessons shown. The presentation will have two companion blog posts with reproducible Python code.\\r\\n\\r\\nThis presentation requires mid-level data science knowledge (e.g. how to train a gradient-boosted trees model) but only beginner Python and finance to follow.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Pedro Tabacof\\nPedro Tabacof is based in Dublin and is currently a staff Machine Learning scientist at Intercom. Previously, he has worked at Wildlife Studios (mobile gaming), Nubank (fintech), iFood (food delivery app). He has used and deployed machine learning models for anti-fraud, credit risk, lifetime value and marketing attribution, using XGBoost or LightGBM in almost all cases. Academically, he has a master's degree in deep learning and 400+ citations.\\n\\n\", metadata={'doc_id': '451066491729346910'}),\n",
       " Document(page_content=\"# Predicting the Spring Classics of cycling with my first neural network\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I‚Äôm back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I‚Äôm attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\n\\n## Description\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I‚Äôm back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I‚Äôm attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I‚Äôll try to provide some insights into the model using SHAP values.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\\n\", metadata={'doc_id': '451066491729346907'}),\n",
       " Document(page_content='# üå≥ The taller the tree, the harder the fall. Determining tree height from space using Deep Learning and very high resolution satellite imagery üõ∞Ô∏è\\nA case study of how we use Deep Learning based photogrammetry to calculate the height of trees from very high resolution satellite imagery. We show the substantial improvement achieved by switching from classical photogrammetric techniques to a deep learning based model (implemented in PyTorch), and the challenges we had to overcome to make this solution work.\\n\\n## Description\\nThe risk that a tree poses to line infrastructure (such as power lines) is determined by several factors, chief among them the height of the particular tree. The increasing availability of very high resolution satellite imagery makes it possible to use photogrammetric techniques to extract height information from a set of stereo satellite images. By using satellite imagery we can achieve a scale not possible by manual measurement. \\r\\nWe found that classical techniques perform poorly on vegetation, and were handily outperformed by deep learning based techniques implemented in PyTorch. This improvement was not trivial to achieve however, as creating labelled data in sufficient quantity was quite challenging. By increasing the quality of our height predictions we were able to more accurately calculate risk for our customers.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Ferdinand Schenck\\nI am a Machine Learning Engineer at LiveEO currently focused on applying Machine Learning techniques to remote sensing data.  \\r\\n\\r\\nBefore that, I did a PhD in particle physics at the Humboldt-Universit√§t zu Berlin on the ATLAS experiment at CERN.\\n\\n', metadata={'doc_id': '451066491729346911'}),\n",
       " Document(page_content='# Causal Forecasting: How to disentangle causal effects, while controlling for unobserved confounders and keeping accuracy\\nA lot of industry-available Machine Learning solutions for causal forecasting have a very particular blind spot: unobserved confounders. We will present an approach that allows you to combine state-of-the-art Machine Learning approaches with advanced Econometrics techniques to get the better of both worlds: accurate causal inference and good forecasting accuracy.\\n\\n## Description\\nCausal Forecasting is a very hot topic in the industry with many applications ranging from marketing spending to pricing. Disentangling causal effects from spurious correlations plays a key role when forecasts are used for decision making, such as in the case of pricing. Solutions available in the industry typically rely on Machine Learning methods that use techniques like DoubleML, Transformers, LSTM, and boosted tree algorithms. A common shortcoming of such solutions is that they do not account for the existence of unobserved confounders, such as world events, or other hard-to-measure effects that can bias the measurement of causal effects. We showcase a solution that was developed over the last 3 years that addresses these challenges by combining advanced Econometrics methods with  ML techniques. The case-study will focus on the example of retail pricing, but the solution is broadly applicable and it has been tested in different settings, including airline pricing.\\n\\n## Timeslot\\n2024-07-11T16:05:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Marc Nientker\\nMarc Nientker transitioned from a successful seven-year academic career in econometrics, where he contributed as a PhD and Assistant Professor, to the business world to apply his knowledge on a broader scale. He co-founded Acmetric, a strategic data science consultancy that focuses on transforming businesses through data-driven insights.\\r\\n\\r\\nAcmetric specializes in practical applications of econometrics in areas such as pricing, inventory optimization, product allocation, measurement, and more. His expertise supports organizations in understanding and implementing data-centric strategies that naturally lead to more informed decision-making and operational efficiencies.\\n\\n', metadata={'doc_id': '451066491729346917'})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"predicting the\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the RAG chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The most relevant talk for you is \"Risks and Mitigations for a Safe and Responsible AI,\" presented by Maria Medina. This talk focuses on the ethical and safety considerations of building AI solutions, aligning with your interest in responsible AI. The other talks focus on evaluating LLM frameworks, running GenAI on local machines, and predicting cycling races using neural networks, which do not directly address responsible AI principles. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\n",
    "    rag_chain.invoke(\n",
    "        \"I'm really into responsible AI. Which talk(s) should I go to?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the full system on previously failing queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Computer vision at the Dutch Tennis Federation: Utilizing YOLO to create insights for coaches\\nThrough single-camera tennis match footage, via a YOLO-driven computer vision system, and culminating in actionable insights for strength and conditioning coaches, the Dutch Tennis Federation offers a pathway for creating tennis data and insights. In our presentation, we will delve into technical specifications and algorithms of our system, navigate through the challenges of working with tennis video footage, and elaborate on our approach to actively engage coaches in our co-creation approach. After the presentation, you will have a deeper understanding of the intricate workings behind implementing such system in a competitive tennis environment. All output of the project will be presented on Github.\\n\\n## Description\\nTennis is seen within the community more as a skill sport than a physical sport. In this way, tennis is an exception compared to other ball sports, in which there is a primary focus on physical data (e.g., distance covered or time in specific speed zones). The primary use of data by now is tactical analysis, scouting your opponent, and finding specific tendencies. Currently, this is done by manually annotating events in game videos for further analysis, which can take up to 5 hours per match. One of the bottlenecks of this process is finding the start of a rally; the effective playing time is actually just 20-30% on clay courts and 10-15% on fast courts. This means that a 5-hour match would have just 30 minutes of playing time that needs to be annotated. Hence, automatically finding the start point of a rally and cutting videos into shorter sequences would tremendously speed up the annotation process and would allow scouting of more players and matches. \\r\\n\\r\\nIn turn, we had two goals in the project. First, to create a solution to optimize the annotation process by providing videos when the ball is in play. The second goal was to provide tennis coaches and players with physical data and to stimulate the use of this type of data (user buy-in). For both of these goals, we faced specific challenges we needed to overcome. Event recognition has been achieved in other sports as well as in tennis using computer vision approaches, especially video tracking (trajectories and coordinates of the players). In tennis, the Hawk-Eye system is used in big tournaments to provide this information. By using 10 synchronized cameras, the system provides player and ball trajectories that enable event recognition and the extraction of physical variables. However, due to the costs of using this system and the complexity of installing it, using it in less prestigious tournaments or for training monitoring is not an option. To overcome this challenge, we created a one-camera computer vision system that allows for player tracking and simple event recognition. \\r\\n\\r\\nAs mentioned earlier, the second challenge of this project is the buy-in by coaches, athletes, and the medical team to physical parameters for athlete monitoring and training optimization. To overcome this problem, we opted for an educational and co-creation approach. This entails, in an initial step, a presentation on the usage of physical data in other sports and their benefits. In a second step, we performed semi-structured interviews with potential end-users (coaches, athletes, medical staff, and performance analysts). Based on these interactive interviews, important variables for the end-users were defined. In addition, potential forms of data presentation and visualizations were discussed in order to create a dashboard for the end-users. In doing so, we improved the understanding of the end-users as well as the commitment to the project. \\r\\n\\r\\nIn this talk, we will provide a summary of the general approach of the conducted interviews and how this resulted in an interactive dashboard for coaches, athletes, and analysts. In addition, we provide an overview of the pipeline of the computer vision approach. While using an ‚Äúoff-the-shelf‚Äù YOLO approach, several processing steps are necessary. This includes several technical challenges like player and court recognition as well as data filtering. We will also provide an example of how we enriched our pipeline with audio data to facilitate event recognition. All in all, we hope to provide an exemplary approach on how to conduct a data science project in a sports environment in which the conceptual barriers between product designer and end-user are often hard to overcome.\\n\\n## Timeslot\\n2024-07-11T14:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Max Brouwer\\nMax works as Data Scientist for the Dutch Tennis Federation (KNLTB). Being part of both the technical staff and the Digital & IT team of the federation, he is involved in many projects for top and recreational tennis. Amongst other things, he works on implementing computer vision & machine learning techniques into match-analysis, on Elo-like rating systems, research, databasing and dashboarding.\\n\\n ### Matthias Kempe\\nDr. Matthias Kempe is an Assistant Professor of Data Science in Sports at the University of Groningen. He received his PhD in Sport Science at the German Sport University Cologne form the Faculty of Exercise Training and Sport Informatics. His research interests include performance optimization and decision making in team sports as well as sports analytics.  He cooperates with different sports federations in Germany and the Netherlands, especially in Handball, Ice-Skating, and Football. Besides that he worked together with Barca Innovation Hub and is a regular mentor for Hackathons (e.g. world data league).   This work has resulted in publications in journals such as Big Data, Journal of Sport Science, European Journal of Sport Science, and Experimental Aging Research\\n\\n', metadata={'doc_id': '451066491729346908'}),\n",
       " Document(page_content='# Cloud? No Thanks! I‚Äôm Gonna Run GenAI on My AI PC\\nIn this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.\\n\\n## Description\\nIn this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master‚Äôs degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he‚Äôs a traveler. You can also talk with him about finance, especially investments.\\n\\n', metadata={'doc_id': '451066491729346918'}),\n",
       " Document(page_content=\"# The Levels of RAG ü¶ú\\nLLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\\n\\n## Description\\nRetrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you ü´µ:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk üìå\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. ‚Ä¶ so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community ü´Ç\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ‚ù§Ô∏è Open Source Software\\r\\nRAG and LLM‚Äôs are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\\n\\n## Timeslot\\n2024-07-11T11:15:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n\\n\", metadata={'doc_id': '451066491729346913'}),\n",
       " Document(page_content='# Maximizing marketplace experimentation: switchback design for small samples and subtle effects\\nConventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.\\n\\n## Description\\nIn this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing\\'s limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic\\'s relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They\\'ll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Jo√´l Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n\\n', metadata={'doc_id': '451066491729346916'})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"Maximizing \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# The Levels of RAG ü¶ú\\nLLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\\n\\n## Description\\nRetrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you ü´µ:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk üìå\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. ‚Ä¶ so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community ü´Ç\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ‚ù§Ô∏è Open Source Software\\r\\nRAG and LLM‚Äôs are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\\n\\n## Timeslot\\n2024-07-11T11:15:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n\\n\", metadata={'doc_id': '451066491729346913'}),\n",
       " Document(page_content='# Cloud? No Thanks! I‚Äôm Gonna Run GenAI on My AI PC\\nIn this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.\\n\\n## Description\\nIn this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master‚Äôs degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he‚Äôs a traveler. You can also talk with him about finance, especially investments.\\n\\n', metadata={'doc_id': '451066491729346918'}),\n",
       " Document(page_content=\"# Scikit-Learn can do THAT?!\\nMany of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\\n\\n## Description\\nThere may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I‚Äôm especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\\n\", metadata={'doc_id': '451066491729346914'}),\n",
       " Document(page_content='# Maximizing marketplace experimentation: switchback design for small samples and subtle effects\\nConventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.\\n\\n## Description\\nIn this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing\\'s limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic\\'s relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They\\'ll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Jo√´l Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n\\n', metadata={'doc_id': '451066491729346916'})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"What's the talk starting with Maximizing about?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The talk titled \"Maximizing marketplace experimentation: switchback design for small samples and subtle effects\" explores switchback design as an alternative to A/B testing. It addresses challenges like small sample sizes and subtle effects in industries like airlines and ride-sharing.  The talk will cover the basics of switchback design, its benefits, and a real-world case study. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\n",
    "    rag_chain.invoke(\n",
    "        \"What's the talk starting with Maximizing about?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# Predicting the Spring Classics of cycling with my first neural network\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I‚Äôm back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I‚Äôm attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\n\\n## Description\\nLast year I attended PyData Eindhoven for the first time. I got inspired and now I‚Äôm back to present my first neural network, a network that was trained to predict the Spring Classics of cycling! With this neural network, I‚Äôm attempting to beat my friends, and myself, in a well-known fantasy cycling game.\\r\\n\\r\\nIn this talk, I will elaborate on the process of building a model from scratch. This will include data collection, model training and finetuning, and of course a discussion of the predicted results. The predictions will also be compared to an existing cycling prediction platform that I use as benchmark. Lastly, I‚Äôll try to provide some insights into the model using SHAP values.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Rob Claessens\\nI'm working as a software engineer at Royal HaskoningDHV, a Dutch consulting and engineering firm. Both professionally and as a hobby, I have been delving into some AI-related subjects. I'm happy to give my first lecture at PyData Eindhoven, to share about my deep dive into Machine Learning, combined with my passion for cycling.\\n\\n\", metadata={'doc_id': '451066491729346907'}),\n",
       " Document(page_content='# Cloud? No Thanks! I‚Äôm Gonna Run GenAI on My AI PC\\nIn this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.\\n\\n## Description\\nIn this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master‚Äôs degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he‚Äôs a traveler. You can also talk with him about finance, especially investments.\\n\\n', metadata={'doc_id': '451066491729346918'}),\n",
       " Document(page_content=\"# Enhancing Event Analysis at Scale: Leveraging Tracking Data in Sports.\\nLearn how to automate the generation of contextual metrics from tracking data to enrich event analysis, handling the influx of games arriving daily in an efficient way by scaling-out the entire architecture.\\n\\n## Description\\nIn the dynamic landscape of sports analytics, the integration of tracking data has opened new frontiers for in-depth event analysis. Yet, the use of this data remains a bottleneck, particularly when dealing with a large volume of games. Indeed, such computation is either too expensive or too long. The focus of the presentation will be on automating the generation of these contextual metrics at scale, and their usage by professionals and decision-makers.\\r\\nThe presentation will showcase an architecture and an automated pipeline designed to handle the influx of games. Leveraging Python and cloud computing services such as message queues, we efficiently manage incoming game data by scaling the infrastructure based on the workload, ensuring optimal performance during peak period while minimizing costs during quieter times. The presentation will strike a balance between technical depth and practical application. Attendees will gain insights into the architecture required to efficiently process hundreds of games weekly, while accommodating the thousands already present in the database. The advantage granted by this method will be quantified in terms of time and resources to inform data scientists and data engineers the efficiency they could reach.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nIf (1.1)\\n\\n## Speaker\\n ### Yannis MOUDERE\\nI am a French Data Scientist, holding an engineering diploma from Telecom Paris and a Master's degree from Institut Polytechnique de Paris in Applied Mathematics and Data Science.\\r\\nAt the end of my studies, I completed a Data Science internship at Parma Calcio 1913. I now serve as a full-time Data Scientist at the club, working on leveraging tracking data.\\n\\n\", metadata={'doc_id': '451066491729346909'}),\n",
       " Document(page_content='# BERTopic to accelerate Ukrainian aid by the Red Cross\\nBy means of Topic Modeling, discussed topics can be subtracted from a set of documents. BERTopic is a way of Topic Modeling that uses Large Language Models. A high-level overview of how BERTopic works will be presented, together with its evaluation and the application of it on a use case of the Netherlands Red Cross. In this use case, BERTopic supports in getting insights into the needs of Ukrainian refugees as expressed through social media.  This presentation will help in understanding BERTopic and might inspire for other valuable use cases within your own field.\\n\\n## Description\\n510, the Data & Digital initiative of the Netherlands Red Cross, is performing Social Media Listening on Telegram channels of Ukrainian refugees, to obtain insights in their needs. As a result, the Red Cross can direct its aid more purposefully. Obtaining such insights can be a time-intensive task, while in case of an emergency, one would like to initialize aid as quickly as possible. BERTopic has shown to be a powerful tool in quickly obtaining an overview of the discussion on social media. \\r\\n\\r\\nIn this presentation, a high-level explanation of BERTopic will be provided, which is a LLM-based method to obtain topics from a set of documents and categorize them accordingly. Other than basic machine learning knowledge, no further background knowledge is required. Rather than explaining how an LLM works, we will focus on what its task in BERTopic is. Next, we will discuss the evaluation of BERTopic, which is a challenging task, as it is an unsupervised method. Three different metrics are proposed, including an innovative approach that creates a test set with GPT. Finally, the application and results of BERTopic to the described use case will be discussed. This will show the practicality of BERTopic and hopefully inspire for other applications. \\r\\n\\r\\nTime breakdown:\\r\\n- Introduction to the Red Cross use case: 5 min\\r\\n- BERTopic: 10 min\\r\\n- Evaluation: 5 min\\r\\n- Application and results: 5 min\\r\\n- Q&A: 5 min\\n\\n## Timeslot\\n2024-07-11T10:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nina van Diermen\\nI recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.\\n\\n ### Nina van Diermen\\nI recently graduated from my master Business Analytics and Operations Research. To complete my master, I wrote my thesis about accelerating Ukrainian Aid by the Red Cross with BERTopic. During PyData, I am happy to tell you more about this cool research project. As of mid June, I started as a Data Scientist at Pipple. In my spare time, I like to play volleyball or go for a run.\\n\\n', metadata={'doc_id': '451066491729346912'})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"I'm gonna run \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# The Levels of RAG ü¶ú\\nLLM's can be supercharged using a technique called RAG, allowing us to overcome dealbreaker problems like hallucinations or no access to internal data. RAG is gaining more industry momentum and is becoming rapidly more mature both in the open-source world and at major Cloud vendors. But what can we expect from RAG? What is the current state of the tech in the industry? What use-cases work well and which are more challenging? Let's find out together!\\n\\n## Description\\nRetrieval Augmented Generation (RAG) is a popular technique to combine retrieval methods like vector search together with Large Language Models (LLM's). This gives us several advantages like retrieving extra information based on a user search query: allowing us to quote and cite LLM-generated answers. Because the underlying techniques are very broadly applicable, many types of data can be used to build up a RAG system, like textual data, tables, graphs or even images.\\r\\n\\r\\nIn this talk, we will deep dive into this popular emerging technique. Together, we will learn about: what the current state of RAG is, what you can expect to work well and what is still very challenging. \\r\\n\\r\\nJoin us if you ü´µ:\\r\\n- Are interested in GenAI / LLM's and RAG\\r\\n- Want to know more about the current state of RAG \\r\\n- Would like to know when you can most successfully apply RAG\\r\\n\\r\\n### Contents of the talk üìå\\r\\n1. [2 min] Intro\\r\\n2. [3 min] Why RAG?\\r\\n    1. The case for RAG\\r\\n    2. The RAG advantage\\r\\n    3. ‚Ä¶ so how-to RAG?\\r\\n3. [5 min] Level 0: Basic RAG\\r\\n    1. Which ingredients make up a successful RAG system?\\r\\n    2. Data ingestion\\r\\n    3. Chunking\\r\\n    4. Vector search\\r\\n    5. Answer generation\\r\\n4. [5 min] Level 1: Hybrid search\\r\\n    1. Combining multiple search methods with Reciprocal Rank Fusion\\r\\n    2. TF-IDF\\r\\n    3. BM-25\\r\\n5. [5 min] Level 2: Advanced data formats\\r\\n    1. The landscape of data formats \\r\\n    2. PDF parsing adventures\\r\\n    3. Tables\\r\\n    4. Graphs\\r\\n4. [5 min] Level 3: Going multimodal\\r\\n5. [4 min] Summing things up\\r\\n    1. The levels of RAG: from basic to advanced\\r\\n    2. GenAI community ü´Ç\\r\\n    3. Concluding remarks\\r\\n6. [1 min] End\\r\\n\\r\\n[30 minutes total]\\r\\n\\r\\n\\r\\n### ‚ù§Ô∏è Open Source Software\\r\\nRAG and LLM‚Äôs are presented in a cloud-agnostic way. Many of the software libraries mentioned are open source. There is no agenda for representing any major cloud.\\n\\n## Timeslot\\n2024-07-11T11:15:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Jeroen Overschie\\nJeroen is a Machine Learning Engineer at Xebia Data (formerly GoDataDriven), in The Netherlands. Jeroen has a background in Software Engineering and Data Science and helps companies take their Machine Learning solutions into production.\\r\\nBesides his usual work, Jeroen has been active in the Open Source community. Jeroen published several PyPi modules, npm modules, and has contributed to several large open source projects (Hydra from Facebook and Emberfire from Google). Jeroen also authored two chrome extensions, which are published on the web store.\\n\\n\", metadata={'doc_id': '451066491729346913'}),\n",
       " Document(page_content='# Cloud? No Thanks! I‚Äôm Gonna Run GenAI on My AI PC\\nIn this speech, we want to introduce an AI PC, a single machine that consists of a CPU, GPU, and NPU (Neural Processing Unit) and can run GenAI in seconds, not hours. Besides the hardware, we will also show the OpenVINO Toolkit, a software solution that helps squeeze as much as possible out of that PC. Join our talk and see for yourself the AI PC is good for both generative and conventional AI models. All presented demos are open source and available on our GitHub.\\n\\n## Description\\nIn this speech, we want to introduce an AI PC, a single machine capable of hosting diverse AI applications that can run GenAI in seconds, not hours. Such an approach for local AI allows us to overcome the usual issues associated with Cloud AI, such as the risk of data privacy breaches, high latency, and dependency on a connection to the cloud.\\r\\n\\r\\nBesides the AI PC, we will also showcase the OpenVINO Toolkit, which is an open-source toolkit for optimizing and deploying deep learning models. It helps to maximize the AI performance of the PC. Join our talk and see for yourself how the AI PC is well-suited for both generative and conventional AI models. All the presented demos, such as background blurring and image generation using a latent consistency model, are open-source and available on our GitHub.\\n\\n## Timeslot\\n2024-07-11T16:50:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Dmitriy Pastushenkov\\nDmitriy Pastushenkov is a passionate AI PC Evangelist at Intel Germany\\xa0\\xa0with more than 20 years of comprehensive and international experience in industrial automation, industrial Internet of Things (IIoT), and real-time operating systems and AI. Dmitriy has held various roles in software development and enablement, software architecture, and technical management.\\xa0\\xa0\\r\\nDmitriy started his career at Intel in 2022 as a Software Architect. He works on the enablement and optimization of real-time, functional safety and AI workloads on the smart edge applying innovative Intel technologies and software products.\\xa0 Currently,\\xa0 as an AI PC Evangelist\\xa0Dmitriy focuses on OpenVINO  and other parts of the AI PC Software Stack.\\r\\nDmitriy has a Master‚Äôs degree in Computer Science from Moscow Power Engineering Institute (Technical University).\\n\\n ### Adrian Boguszewski\\nAI Software Evangelist at Intel. Adrian graduated from the Gdansk University of Technology in the field of Computer Science 7 years ago. After that, he started his career in computer vision and deep learning. As a team leader of data scientists and Android developers for the previous two years, Adrian was responsible for an application to take a professional photo (for an ID card or passport) without leaving home. He is a co-author of the LandCover.ai dataset, creator of the OpenCV Image Viewer Plugin, and a Deep Learning lecturer occasionally. His current role is to educate people about OpenVINO Toolkit. In his free time, he‚Äôs a traveler. You can also talk with him about finance, especially investments.\\n\\n', metadata={'doc_id': '451066491729346918'}),\n",
       " Document(page_content=\"# Scikit-Learn can do THAT?!\\nMany of us know scikit-learn for it's ability to construct pipelines that can do .fit().predict(). It's an amazing feature for sure. But once you dive into the codebase ... you realise that there is just so much more. \\r\\n\\r\\nThis talk will be an attempt at demonstrating some extra features in scikit-learn, and it's ecosystem, that are less common but deserve to be in the spotlight. \\r\\n\\r\\nIn particular I hope to discuss these things that scikit-learn can do:\\r\\n\\r\\n- sparse datasets and models\\r\\n- larger than memory datasets\\r\\n- sample weight techniques\\r\\n- image classification via embeddings\\r\\n- tabular embeddings/vectorisation \\r\\n- data deduplication\\r\\n- pipeline caching\\r\\n\\r\\nIf time allows I may also touch on extra topics.\\n\\n## Description\\nThere may be an opportunity to live code some of these examples, but if live coding is not possible it'd be preferable to know this ahead of time.\\n\\n## Timeslot\\n2024-07-11T12:00:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Vincent D. Warmerdam\\nVincent is a senior data professional, and recovering consultant, who worked as an engineer, researcher, team lead, and educator in the past. I‚Äôm especially interested in understanding algorithmic systems so that one may prevent failure. As such, he prefers simpler solutions that scale and worry more about data quality than the number of tensors we throw at a problem. He's also well known for creating calmcode as well as a small dozen of open-source packages.\\r\\n\\r\\nHe's currently employed at probabl where he works together with scikit-learn core maintainers to improve the ecosystem of tooling.\\n\\n\", metadata={'doc_id': '451066491729346914'}),\n",
       " Document(page_content='# Maximizing marketplace experimentation: switchback design for small samples and subtle effects\\nConventional A/B testing often falls short in industries such as airlines, ride-sharing, and delivery services, where challenges like small samples and subtle effects complicate testing new features. Inspired by its significant impact in leading companies like Uber, Lyft, and Doordash, we introduce the switchback design as a practical alternative to conventional A/B testing. By addressing small sample size limitations and the need to detect subtle effects quickly, this approach boosts statistical power while reducing variability and interference. We guide the audience through the challenges of marketplace experimentation and implementing this approach, period length optimization and switch frequency using a case study from the airline industry.\\n\\n## Description\\nIn this talk, we introduce switchback design, a method that addresses key challenges in marketplace experimentation faced by sectors like airlines and ride-sharing. It addresses traditional A/B testing\\'s limitations on small samples and subtle effects, highlighted by successes in companies like Uber, Lyft, and Doordash. This approach, crucial for accurate effect measurement, aims to boost decision-making and operational optimization, supported by a practical case study.\\r\\n\\r\\nTalk outline: \\r\\n- Introduction (5 mins): Quick intro to our expertise and the topic\\'s relevance. \\r\\n- Challenges (5 mins): Overview of marketplace experimentation challenges. \\r\\n- Switchback Design (5 mins): Basics of switchback design vs. traditional A/B testing. \\r\\n- Case Study (10 mins): Real-world application in the airline industry, showcasing benefits and drawbacks. \\r\\n- Key Takeaways (5 mins): Summary of our talk. \\r\\n- Q&A (10 mins): Open discussion for clarifications and further insights.\\r\\n\\r\\nThe intended audience: Targeted at data scientists, data analysts, product managers, and anyone interested in data-driven decision-making. Ideal for those curious about experimentation and causal inference in sectors like tech and e-commerce.\\r\\n\\r\\nNo background needed: Open to all levels, no prior knowledge needed. Concepts will be explained simply, focusing on practical insights without complex math.\\r\\n\\r\\nThe takeaway for the audience: The audience will understand the effectiveness of switchback design in overcoming marketplace experimentation hurdles, such as dealing with small samples and subtle effects. They\\'ll learn from the success stories of top firms, understand practical steps for executing switchback experiments, and recognize its advantages over conventional A/B testing for more informed decisions and improved results.\\n\\n## Timeslot\\n2024-07-11T14:45:00+02:00 with a duration of 00:30\\n\\n## Room\\nElse (1.3)\\n\\n## Speaker\\n ### Nazli M. Alagoz\\nNaz is a data scientist at ACMetric, a boutique Data Science & Artificial Intelligence Consulting firm. She specializes in leveraging causal inference and machine learning to improve experimentation and analysis. Alongside her work, she is in her final stages of Ph.D. in Quantitative Marketing. She is skilled in Python and R, turning complex data into clear insights and recommendations for stakeholders. Passionate about reproducible science, she is a data science blogger and speaker.\\n\\n ### Jo√´l Gastelaars\\nWorking on the edge of Business Strategy and Data Science. For me, it is all about converting business challenges into scalable data (science) solutions to create tangible value for the client. Currently leading the implementation of a company-wide experimentation & measurement platform in the airline industry and consulting on organizational change.\\r\\n\\r\\nBesides being a \"techy\", I like to make sure everyone understands what we do, why we do it and how it adds value for their business.\\n\\n', metadata={'doc_id': '451066491729346916'})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"What's the talk starting with Maximizing about?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levels-of-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
